"""
Streamlit app for Diabetes Classification and EDA
================================================

This app provides an interactive interface to explore a health dataset and
train various classification algorithms.  It performs basic exploratory
data analysis (EDA) on numeric and categorical features, displays summary
statistics, and visualises distributions.  Users can then choose a
classifier, configure a simple hyperparameter search and run the
experiment.  After training, the app presents the best parameters,
classification metrics, confusion matrix and ROC curves.

The implementation is designed to be robust: missing optional
dependencies (`ucimlrepo`, `imblearn`) are handled gracefully, and
reasonable fallbacks ensure that the app can still function even when
certain libraries or datasets are unavailable.  Large datasets are
optionally down‑sampled to avoid long training times.

To run the application locally use the command:

```
streamlit run app.py
```

Make sure that the required Python packages are installed.  At a minimum
you will need `streamlit`, `pandas`, `numpy`, `matplotlib`, and
`scikit‑learn`.  Optional dependencies include `imblearn` for SMOTE
oversampling and `ucimlrepo` to load the original UCI dataset used in
the accompanying notebook.

Author: ChatGPT
"""

import warnings
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt

import streamlit as st

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectFromModel
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    roc_curve,
    auc,
)
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    HistGradientBoostingClassifier,
    ExtraTreesClassifier,
)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# Attempt to import imblearn for SMOTE.  If unavailable, we fall back to
# pipelines without oversampling.
try:
    from imblearn.over_sampling import SMOTE  # type: ignore
    from imblearn.pipeline import Pipeline as ImbPipeline  # type: ignore
except Exception:
    SMOTE = None  # type: ignore
    ImbPipeline = None  # type: ignore

# Disable global pyplot deprecation warnings from Streamlit where supported.
# In some Streamlit versions the configuration key may not be recognised.  To
# avoid raising an exception on import we wrap the call in a try/except block.
try:
    st.set_option("deprecation.showPyplotGlobalUse", False)
except Exception:
    # Fall back silently if the option is unavailable.
    pass


class AdaptiveTruncatedSVD:
    """
    Fit a TruncatedSVD such that the cumulative explained variance
    exceeds a user specified threshold.  This transformer is useful on
    high dimensional sparse matrices generated by one‑hot encoding.  It
    determines the number of components required to reach the target
    variance, fits an SVD with that many components and stores the
    necessary attributes for downstream inspection.

    Parameters
    ----------
    target_variance : float, default=0.90
        The minimum cumulative variance that should be explained by the
        retained components.  Values should be within (0, 1].

    max_components : int, default=300
        Upper bound on the number of components to attempt during the
        exploratory fit.  The algorithm will never retain more than this
        many components.

    random_state : int, default=42
        Random state passed to the internal TruncatedSVD estimator.
    """

    def __init__(
        self, target_variance: float = 0.90, max_components: int = 300, random_state: int = 42
    ) -> None:
        self.target_variance = target_variance
        self.max_components = max_components
        self.random_state = random_state
        self.n_components_: Optional[int] = None
        self.svd_: Optional[TruncatedSVD] = None
        self.explained_variance_ratio_: Optional[np.ndarray] = None
        self.cumulative_variance_: Optional[np.ndarray] = None

    def fit(self, X: Any, y: Optional[Any] = None) -> "AdaptiveTruncatedSVD":
        # Determine maximum allowable components based on input dimensionality
        n_features = X.shape[1]
        max_allow = max(2, min(self.max_components, n_features - 1))
        # Initial probe to estimate cumulative variance across many components
        svd_probe = TruncatedSVD(n_components=max_allow, random_state=self.random_state)
        svd_probe.fit(X)
        cum = np.cumsum(svd_probe.explained_variance_ratio_)
        self.cumulative_variance_ = cum
        # Identify the number of components needed to exceed target_variance
        idx_candidates = np.where(cum >= self.target_variance)[0]
        if len(idx_candidates) > 0:
            k = int(idx_candidates[0] + 1)
        else:
            k = max_allow
        self.n_components_ = k
        # Fit final SVD with determined number of components
        self.svd_ = TruncatedSVD(n_components=k, random_state=self.random_state)
        self.svd_.fit(X)
        self.explained_variance_ratio_ = self.svd_.explained_variance_ratio_
        return self

    def transform(self, X: Any) -> Any:
        if self.svd_ is None:
            raise RuntimeError("AdaptiveTruncatedSVD instance is not fitted yet.")
        return self.svd_.transform(X)

    def fit_transform(self, X: Any, y: Optional[Any] = None) -> Any:
        return self.fit(X).transform(X)

    def get_feature_names_out(self, input_features: Optional[List[str]] = None) -> np.ndarray:
        if self.n_components_ is None:
            raise RuntimeError("AdaptiveTruncatedSVD instance is not fitted yet.")
        return np.array([f"svd_{i+1}" for i in range(self.n_components_)])


@st.cache_data(show_spinner=False)
def load_data(max_rows: Optional[int] = None) -> Tuple[pd.DataFrame, pd.Series, List[str], List[str]]:
    """
    Load a health dataset for classification.  Attempt to fetch the UCI
    diabetes dataset via `ucimlrepo`; if that fails (due to missing
    package or network issues), fall back to scikit‑learn's breast
    cancer dataset.  Returns features, labels, numeric column names and
    categorical column names.  Sampling is performed if `max_rows` is
    provided and the dataset exceeds this size.

    Parameters
    ----------
    max_rows : int, optional
        If provided and the dataset contains more than `max_rows` rows,
        a random subset of this size will be returned.

    Returns
    -------
    X : pandas.DataFrame
        Feature matrix.

    y : pandas.Series
        Target vector (string labels).

    num_cols : list of str
        Names of numeric columns in X.

    cat_cols : list of str
        Names of categorical columns in X.
    """
    # Attempt to load the diabetes dataset from UCI repository via ucimlrepo
    X: Optional[pd.DataFrame] = None
    y: Optional[pd.Series] = None
    try:
        from ucimlrepo import fetch_ucirepo  # type: ignore

        ds = fetch_ucirepo(id=296)
        X = ds.data.features.copy()
        y = ds.data.targets.copy().iloc[:, 0].astype(str)
        # Drop high cardinality identifiers and codes to prevent leaks
        drop_cols = [
            c
            for c in X.columns
            if c.lower()
            in (
                "encounter_id",
                "patient_nbr",
                "encounterid",
                "patientnbr",
                "diag_1",
                "diag_2",
                "diag_3",
                "payer_code",
                "medical_specialty",
                "weight",
                "max_glu_serum",
                "a1cresult",
            )
        ]
        X = X.drop(columns=drop_cols, errors="ignore")
        # Provide a note if sampling is requested and dataset is large
        if max_rows is not None and len(X) > max_rows:
            rng = np.random.default_rng(42)
            idx = rng.choice(X.index, size=max_rows, replace=False)
            X = X.loc[idx].reset_index(drop=True)
            y = y.loc[idx].reset_index(drop=True)
    except Exception:
        # Fallback to scikit‑learn breast cancer dataset
        from sklearn.datasets import load_breast_cancer

        data = load_breast_cancer(as_frame=True)
        X = data.data.copy()
        y = pd.Series(data.target).astype(str)
        X.columns = [c.replace(" ", "_").lower() for c in X.columns]
        y.name = "target"
        if max_rows is not None and len(X) > max_rows:
            rng = np.random.default_rng(42)
            idx = rng.choice(X.index, size=max_rows, replace=False)
            X = X.loc[idx].reset_index(drop=True)
            y = y.loc[idx].reset_index(drop=True)
    # Identify numeric and categorical columns
    num_cols = X.select_dtypes(include=np.number).columns.tolist()
    cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()
    return X, y, num_cols, cat_cols


def build_preprocessor(num_cols: List[str], cat_cols: List[str]) -> ColumnTransformer:
    """
    Construct a preprocessor ColumnTransformer that imputes missing values,
    scales numeric features and one‑hot encodes categorical features.  The
    output is sparse whenever categorical features are present.

    Parameters
    ----------
    num_cols : list of str
        Names of numeric columns to be imputed and scaled.

    cat_cols : list of str
        Names of categorical columns to be imputed and encoded.

    Returns
    -------
    preprocessor : ColumnTransformer
        A transformer ready to be used in a pipeline.
    """
    # Numeric pipeline: median imputation followed by standard scaling
    num_pipeline = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler(with_mean=True)),
        ]
    )
    # Categorical pipeline: most frequent imputation followed by one‑hot encoding
    try:
        ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=True)
    except TypeError:
        # Older versions of scikit‑learn use `sparse` parameter
        ohe = OneHotEncoder(handle_unknown="ignore", sparse=True)
    cat_pipeline = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("encoder", ohe),
        ]
    )
    return ColumnTransformer(
        transformers=[
            ("num", num_pipeline, num_cols),
            ("cat", cat_pipeline, cat_cols),
        ],
        sparse_threshold=1.0,
    )


def get_models() -> Dict[str, Any]:
    """
    Return a dictionary mapping human friendly model names to sklearn
    classifier instances.  Additional models can be added here.
    """
    return {
        "RandomForest": RandomForestClassifier(random_state=42),
        "GradientBoosting": GradientBoostingClassifier(random_state=42),
        "HistGradientBoosting": HistGradientBoostingClassifier(random_state=42),
        "ExtraTrees": ExtraTreesClassifier(random_state=42),
        "KNN": KNeighborsClassifier(),
        "DecisionTree": DecisionTreeClassifier(random_state=42),
    }


def get_param_distributions(model_name: str) -> Dict[str, List[Any]]:
    """
    Provide a small hyperparameter grid for each classifier.  Random
    distributions from scipy.stats are intentionally avoided here to
    eliminate the dependency on SciPy.  Each grid contains a few
    reasonable candidate values for core hyperparameters.  Users may
    modify these grids or provide their own.

    Parameters
    ----------
    model_name : str
        Name of the classifier (key in get_models()).

    Returns
    -------
    param_grid : dict
        Dictionary with parameter names as keys and lists of values to
        sample during the search.
    """
    if model_name == "RandomForest":
        return {
            "classifier__n_estimators": [100, 200],
            "classifier__max_depth": [None, 10, 20],
            "classifier__min_samples_split": [2, 5, 10],
            "classifier__min_samples_leaf": [1, 2, 4],
            "classifier__max_features": ["sqrt", "log2", None],
            "classifier__bootstrap": [True, False],
        }
    elif model_name == "GradientBoosting":
        return {
            "classifier__n_estimators": [50, 100, 150],
            "classifier__learning_rate": [0.01, 0.05, 0.1],
            "classifier__max_depth": [3, 5, 7],
            "classifier__min_samples_split": [2, 5, 10],
            "classifier__min_samples_leaf": [1, 2, 4],
        }
    elif model_name == "HistGradientBoosting":
        return {
            "classifier__max_iter": [50, 100, 150],
            "classifier__learning_rate": [0.01, 0.05, 0.1],
            "classifier__max_depth": [None, 10, 20],
            "classifier__max_leaf_nodes": [15, 31, 63],
        }
    elif model_name == "ExtraTrees":
        return {
            "classifier__n_estimators": [100, 200],
            "classifier__max_depth": [None, 10, 20],
            "classifier__min_samples_split": [2, 5, 10],
            "classifier__min_samples_leaf": [1, 2, 4],
            "classifier__max_features": ["sqrt", "log2", None],
            "classifier__bootstrap": [True, False],
        }
    elif model_name == "KNN":
        return {
            "classifier__n_neighbors": [3, 5, 7, 9, 11],
            "classifier__weights": ["uniform", "distance"],
            "classifier__p": [1, 2],
        }
    elif model_name == "DecisionTree":
        return {
            "classifier__criterion": ["gini", "entropy"],
            "classifier__max_depth": [None, 10, 20],
            "classifier__min_samples_split": [2, 5, 10],
            "classifier__min_samples_leaf": [1, 2, 4],
            "classifier__ccp_alpha": [0.0, 0.005, 0.01],
        }
    # Default empty grid for unknown models
    return {}


def build_pipeline(model_name: str, preprocessor: ColumnTransformer) -> Any:
    """
    Construct a model pipeline including preprocessing, optional SMOTE
    oversampling, feature selection, dimensionality reduction and the
    classifier itself.  The pipeline uses `imblearn`'s Pipeline if
    available to correctly handle oversampling; otherwise a standard
    scikit‑learn Pipeline is used.  If SMOTE is unavailable, the
    oversampling step is skipped entirely.

    Parameters
    ----------
    model_name : str
        Name of the classifier to be instantiated.

    preprocessor : ColumnTransformer
        Preprocessor returned by `build_preprocessor`.

    Returns
    -------
    pipeline : sklearn or imblearn Pipeline
        A composite estimator ready for fitting.
    """
    models = get_models()
    if model_name not in models:
        raise ValueError(f"Unknown model '{model_name}'.")
    classifier = models[model_name]
    # Feature selection using ExtraTrees to keep important variables
    selector = SelectFromModel(
        estimator=ExtraTreesClassifier(n_estimators=200, random_state=42, n_jobs=-1),
        threshold="median",
    )
    reducer = AdaptiveTruncatedSVD(target_variance=0.90, max_components=100, random_state=42)
    steps: List[Tuple[str, Any]] = []
    steps.append(("preprocessor", preprocessor))
    # Include oversampling if imblearn is available and dataset is imbalanced
    if SMOTE is not None and ImbPipeline is not None:
        steps.append(("sampler", SMOTE(random_state=42)))
    steps.append(("selector", selector))
    steps.append(("reducer", reducer))
    steps.append(("classifier", classifier))
    # Choose appropriate Pipeline class
    PipelineClass = ImbPipeline if (SMOTE is not None and ImbPipeline is not None) else Pipeline
    return PipelineClass(steps=steps)


def train_model(
    pipeline: Any,
    param_grid: Dict[str, List[Any]],
    X_train: pd.DataFrame,
    y_train: np.ndarray,
    n_iter: int = 5,
    cv: int = 3,
    random_state: int = 42,
) -> RandomizedSearchCV:
    """
    Fit a RandomizedSearchCV on the provided pipeline and training data.

    Parameters
    ----------
    pipeline : sklearn or imblearn Pipeline
        The pipeline to be optimised.

    param_grid : dict
        Parameter search space.

    X_train : pandas.DataFrame
        Training features.

    y_train : numpy.ndarray
        Encoded training labels.

    n_iter : int, default=5
        Number of parameter settings that are sampled.

    cv : int, default=3
        Number of cross‑validation folds.

    random_state : int, default=42
        Seed for reproducibility.

    Returns
    -------
    search : RandomizedSearchCV
        Fitted search object containing best estimator and results.
    """
    search = RandomizedSearchCV(
        estimator=pipeline,
        param_distributions=param_grid,
        n_iter=n_iter,
        cv=cv,
        verbose=1,
        n_jobs=-1,
        random_state=random_state,
    )
    search.fit(X_train, y_train)
    return search


def plot_confusion_matrix(cm: np.ndarray, class_names: List[str], model_name: str) -> None:
    """
    Plot and display a confusion matrix using matplotlib and Streamlit.

    Parameters
    ----------
    cm : numpy.ndarray
        Confusion matrix returned by sklearn.metrics.confusion_matrix.

    class_names : list of str
        Names of the classes corresponding to rows/columns of cm.

    model_name : str
        Identifier for the plot title.
    """
    fig, ax = plt.subplots(figsize=(6, 5))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    disp.plot(cmap="Blues", values_format="d", ax=ax, colorbar=False)
    ax.set_title(f"Matriz de Confusión — {model_name}")
    plt.tight_layout()
    st.pyplot(fig)


def plot_roc_curves(
    y_true: np.ndarray,
    y_score: np.ndarray,
    class_names: List[str],
    model_name: str,
) -> None:
    """
    Plot ROC curves for multi‑class or binary classification problems.

    Parameters
    ----------
    y_true : numpy.ndarray
        True labels encoded as integers from 0 to n_classes-1.

    y_score : numpy.ndarray
        Predicted probabilities or decision scores of shape (n_samples, n_classes).

    class_names : list of str
        Names of the classes.

    model_name : str
        Identifier for the plot title.
    """
    n_classes = len(class_names)
    # Binarize the output for multiclass
    from sklearn.preprocessing import label_binarize

    y_test_bin = label_binarize(y_true, classes=np.arange(n_classes))
    plt.figure(figsize=(7, 6))
    plotted = 0
    for i in range(n_classes):
        # Skip classes not present in the test set
        if len(np.unique(y_test_bin[:, i])) < 2:
            continue
        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=1.5, label=f"{class_names[i]} (AUC = {roc_auc:.2f})")
        plotted += 1
    if plotted == 0:
        st.warning("No hay clases suficientes en el conjunto de prueba para trazar curvas ROC.")
        return
    plt.plot([0, 1], [0, 1], linestyle="--", lw=1)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("Tasa de Falsos Positivos")
    plt.ylabel("Tasa de Verdaderos Positivos")
    plt.title(f"Curvas ROC (One‑vs‑Rest) — {model_name}")
    plt.legend(loc="lower right")
    plt.tight_layout()
    st.pyplot(plt.gcf())


def main() -> None:
    """
    Entry point for the Streamlit app.  This function coordinates data
    loading, EDA visualisation and model training based on user inputs.
    """
    st.title("Clasificación de Diabetes y Análisis Exploratorio")
    st.write(
        "Esta aplicación permite explorar un conjunto de datos de salud y "
        "entrenar distintos clasificadores con búsqueda de hiperparámetros. "
        "Si el conjunto de datos original de la UCI no está disponible, se "
        "usará un conjunto de datos alternativo (cáncer de mama)."
    )

    # Sidebar for navigation
    page = st.sidebar.selectbox(
        "Seleccione la sección",
        ["Exploración de datos", "Entrenamiento de modelos"],
    )

    # Sidebar configuration options
    max_sample = st.sidebar.number_input(
        "Máximo de filas a cargar (0 = todas)",
        min_value=0,
        value=0,
        step=100,
        help="Limita el tamaño del conjunto de datos para acelerar el entrenamiento."
    )
    max_rows = int(max_sample) if max_sample > 0 else None

    # Load data (cached)
    with st.spinner("Cargando datos..."):
        X, y, num_cols, cat_cols = load_data(max_rows=max_rows)
    if X is None or y is None:
        st.error("No se pudieron cargar los datos.")
        return

    # Encode target labels for modelling
    le = LabelEncoder().fit(y)
    y_enc = le.transform(y)
    class_names = le.classes_.tolist()

    if page == "Exploración de datos":
        st.header("Exploración de datos")
        st.subheader("Información general")
        st.write(f"Número de filas: {len(X)}")
        st.write(f"Número de columnas: {len(X.columns)}")

        # Display numeric/categorical counts
        st.write(
            f"Variables numéricas: {len(num_cols)} — {', '.join(num_cols) if num_cols else 'Ninguna'}"
        )
        st.write(
            f"Variables categóricas: {len(cat_cols)} — {', '.join(cat_cols) if cat_cols else 'Ninguna'}"
        )

        # Unique category counts
        if cat_cols:
            st.subheader("Número de clases únicas por variable categórica")
            cat_counts = {col: X[col].nunique() for col in cat_cols}
            cat_df = pd.DataFrame(
                {
                    "Variable": list(cat_counts.keys()),
                    "Clases únicas": list(cat_counts.values()),
                }
            ).sort_values("Clases únicas", ascending=False)
            st.dataframe(cat_df, hide_index=True)

        # Missing values analysis
        st.subheader("Valores faltantes")
        missing = X.isnull().sum()
        missing = missing[missing > 0].sort_values(ascending=False)
        if not missing.empty:
            missing_df = pd.DataFrame(
                {
                    "Variable": missing.index,
                    "Cantidad faltante": missing.values,
                    "Porcentaje (%)": (missing.values / len(X)) * 100,
                }
            )
            st.dataframe(missing_df, hide_index=True)
        else:
            st.write("No se encontraron valores faltantes en el conjunto de datos.")

        # Distribution of target
        st.subheader("Distribución de la variable objetivo")
        target_dist = pd.Series(y).value_counts().sort_index()
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.bar(target_dist.index.astype(str), target_dist.values)
        ax.set_xlabel("Clase")
        ax.set_ylabel("Frecuencia")
        ax.set_title("Distribución de Clases en la variable objetivo")
        st.pyplot(fig)

        # Histograms for numeric features
        if num_cols:
            st.subheader("Histogramas de variables numéricas")
            selected_num = st.multiselect(
                "Seleccione variables numéricas para visualizar",
                num_cols,
                default=num_cols[: min(3, len(num_cols))],
            )
            if selected_num:
                fig, axes = plt.subplots(
                    nrows=len(selected_num), ncols=1, figsize=(6, 3 * len(selected_num))
                )
                if len(selected_num) == 1:
                    axes = [axes]
                for ax, col in zip(axes, selected_num):
                    ax.hist(X[col].dropna(), bins=20, color="tab:blue", edgecolor="white")
                    ax.set_title(f"Distribución de {col}")
                    ax.set_xlabel(col)
                    ax.set_ylabel("Frecuencia")
                plt.tight_layout()
                st.pyplot(fig)

        # Bar charts for categorical features
        if cat_cols:
            st.subheader("Conteo de variables categóricas")
            selected_cat = st.multiselect(
                "Seleccione variables categóricas para visualizar",
                cat_cols,
                default=cat_cols[: min(3, len(cat_cols))],
            )
            for col in selected_cat:
                fig, ax = plt.subplots(figsize=(6, 3))
                value_counts = X[col].value_counts().head(30)
                ax.bar(value_counts.index.astype(str), value_counts.values)
                ax.set_title(f"Distribución de '{col}'")
                ax.set_xlabel(col)
                ax.set_ylabel("Frecuencia")
                plt.xticks(rotation=45, ha="right")
                st.pyplot(fig)

    elif page == "Entrenamiento de modelos":
        st.header("Entrenamiento de modelos")
        st.write(
            "Seleccione el clasificador, ajuste el número de iteraciones de la búsqueda "
            "aleatoria y ejecute el entrenamiento. Para conjuntos de datos grandes se "
            "recomienda reducir el número de iteraciones para acelerar el proceso."
        )

        # Model selection
        model_options = list(get_models().keys())
        model_name = st.selectbox("Clasificador", model_options)

        # Hyperparameter search settings
        n_iter = st.slider(
            "Número de configuraciones a probar (RandomizedSearchCV)",
            min_value=1,
            max_value=10,
            value=5,
            step=1,
            help="Más iteraciones incrementan el tiempo de entrenamiento pero pueden mejorar el rendimiento.",
        )
        cv_folds = st.slider(
            "Número de particiones de validación cruzada", min_value=2, max_value=5, value=3, step=1
        )
        # Trigger training
        if st.button("Entrenar modelo"):
            with st.spinner("Entrenando el modelo, por favor espere..."):
                try:
                    preprocessor = build_preprocessor(num_cols, cat_cols)
                    pipeline = build_pipeline(model_name, preprocessor)
                    param_grid = get_param_distributions(model_name)
                    # Perform train/test split
                    X_train, X_test, y_train_enc, y_test_enc = train_test_split(
                        X, y_enc, test_size=0.25, stratify=y_enc, random_state=42
                    )
                    search = train_model(
                        pipeline,
                        param_grid,
                        X_train,
                        y_train_enc,
                        n_iter=n_iter,
                        cv=cv_folds,
                        random_state=42,
                    )
                    # Predictions and evaluation on test set
                    best_estimator = search.best_estimator_
                    y_pred = best_estimator.predict(X_test)
                    report_dict = classification_report(
                        y_test_enc, y_pred, target_names=class_names, output_dict=True
                    )
                    # Display best parameters and classification report
                    st.subheader("Mejores hiperparámetros encontrados")
                    st.json(search.best_params_)
                    st.subheader("Informe de clasificación (test)")
                    report_df = pd.DataFrame(report_dict).T
                    st.dataframe(report_df.style.format({
                        "precision": "{:.3f}",
                        "recall": "{:.3f}",
                        "f1-score": "{:.3f}",
                        "support": "{:.0f}",
                    }))
                    # Confusion Matrix
                    cm = confusion_matrix(y_test_enc, y_pred)
                    st.subheader("Matriz de confusión")
                    plot_confusion_matrix(cm, class_names, model_name)
                    # ROC Curves if applicable
                    # Determine probabilities
                    y_score: Optional[np.ndarray] = None
                    try:
                        if hasattr(best_estimator, "predict_proba"):
                            y_score = best_estimator.predict_proba(X_test)
                    except Exception:
                        y_score = None
                    if y_score is None:
                        try:
                            if hasattr(best_estimator, "decision_function"):
                                y_score = best_estimator.decision_function(X_test)
                        except Exception:
                            y_score = None
                    if y_score is not None:
                        st.subheader("Curvas ROC One‑vs‑Rest")
                        # Ensure y_score has shape (n_samples, n_classes)
                        if y_score.ndim == 1:
                            # Binary classifiers with decision_function may return 1D
                            y_score = np.column_stack([1 - y_score, y_score])
                        # Align columns with class order
                        if hasattr(best_estimator, "classes_"):
                            est_classes = best_estimator.classes_
                        else:
                            est_classes = np.arange(y_score.shape[1])
                        order_idx = [
                            int(np.where(est_classes == i)[0][0]) if i in est_classes else i
                            for i in range(len(class_names))
                        ]
                        try:
                            y_score = y_score[:, order_idx]
                        except Exception:
                            pass
                        plot_roc_curves(y_test_enc, y_score, class_names, model_name)
                    else:
                        st.info(
                            "El modelo no proporciona probabilidades ni puntajes de decisión, por lo que no se pueden trazar curvas ROC."
                        )
                except Exception as e:
                    st.error(f"Se produjo un error durante el entrenamiento: {e}")


if __name__ == "__main__":
    # Catch warnings and convert them to messages in Streamlit where appropriate
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        main()