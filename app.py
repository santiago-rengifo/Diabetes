# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aOt9CRonp7QMS4ekEucePpOVlGGBWOWL
"""

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
from collections import Counter

# Configuraci√≥n inicial de la p√°gina
st.set_page_config(
    page_title="An√°lisis de Diabetes - ML Pipeline",
    page_icon="ü©∫",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Importaciones de ML
@st.cache_resource
def import_ml_libraries():
    # UCI Repo
    from ucimlrepo import fetch_ucirepo

    # Preprocesamiento
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.impute import SimpleImputer
    from sklearn.compose import ColumnTransformer

    # Reducci√≥n de dimensionalidad
    from sklearn.decomposition import PCA

    # Selecci√≥n de caracter√≠sticas
    from sklearn.feature_selection import chi2, f_classif, SelectKBest, RFECV

    # Modelos de clasificaci√≥n
    from sklearn.ensemble import (
        RandomForestClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier
    )
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC

    # Validaci√≥n y optimizaci√≥n
    from sklearn.model_selection import train_test_split, RandomizedSearchCV

    # M√©tricas
    from sklearn.metrics import (
        classification_report, confusion_matrix, ConfusionMatrixDisplay,
        roc_curve, auc, RocCurveDisplay, accuracy_score, f1_score
    )

    # Pipelines
    from sklearn.pipeline import Pipeline
    try:
        from imblearn.pipeline import Pipeline as ImbPipeline
        from imblearn.over_sampling import ADASYN, SMOTE
        from sklearn.utils.class_weight import compute_class_weight
        IMBALANCED_AVAILABLE = True
    except ImportError:
        IMBALANCED_AVAILABLE = False

    # Estad√≠stica
    from scipy.stats import spearmanr, randint, uniform

    return {
        'fetch_ucirepo': fetch_ucirepo,
        'StandardScaler': StandardScaler,
        'LabelEncoder': LabelEncoder,
        'SimpleImputer': SimpleImputer,
        'ColumnTransformer': ColumnTransformer,
        'PCA': PCA,
        'chi2': chi2,
        'f_classif': f_classif,
        'SelectKBest': SelectKBest,
        'RFECV': RFECV,
        'RandomForestClassifier': RandomForestClassifier,
        'ExtraTreesClassifier': ExtraTreesClassifier,
        'HistGradientBoostingClassifier': HistGradientBoostingClassifier,
        'LogisticRegression': LogisticRegression,
        'SVC': SVC,
        'train_test_split': train_test_split,
        'RandomizedSearchCV': RandomizedSearchCV,
        'classification_report': classification_report,
        'confusion_matrix': confusion_matrix,
        'ConfusionMatrixDisplay': ConfusionMatrixDisplay,
        'roc_curve': roc_curve,
        'auc': auc,
        'accuracy_score': accuracy_score,
        'f1_score': f1_score,
        'Pipeline': Pipeline,
        'randint': randint,
        'uniform': uniform,
        'IMBALANCED_AVAILABLE': IMBALANCED_AVAILABLE
    }

# Cargar librer√≠as
ml_libs = import_ml_libraries()

# Configuraci√≥n global
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# CSS personalizado
st.markdown("""
<style>
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 5px solid #ff6b6b;
    }
    .success-card {
        background-color: #d4edda;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 5px solid #28a745;
    }
    .warning-card {
        background-color: #fff3cd;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 5px solid #ffc107;
    }
    .error-card {
        background-color: #f8d7da;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 5px solid #dc3545;
    }
</style>
""", unsafe_allow_html=True)

def main():
    st.title("ü©∫ An√°lisis de Diabetes - Pipeline de Machine Learning")
    st.markdown("---")

    # Sidebar para navegaci√≥n
    st.sidebar.title("üîß Panel de Control")

    # Secci√≥n principal
    page = st.sidebar.selectbox(
        "Selecciona una secci√≥n:",
        [
            "üìä Carga y Exploraci√≥n de Datos",
            "üîß Preprocesamiento",
            "üìâ Selecci√≥n de Caracter√≠sticas",
            "ü§ñ Modelado Individual",
            "üèÜ Comparaci√≥n de Modelos",
            "üìà Resultados y Visualizaciones"
        ]
    )

    if page == "üìä Carga y Exploraci√≥n de Datos":
        seccion_carga_datos()
    elif page == "üîß Preprocesamiento":
        seccion_preprocesamiento()
    elif page == "üìâ Selecci√≥n de Caracter√≠sticas":
        seccion_seleccion_caracteristicas()
    elif page == "ü§ñ Modelado Individual":
        seccion_modelado_individual()
    elif page == "üèÜ Comparaci√≥n de Modelos":
        seccion_comparacion_modelos()
    elif page == "üìà Resultados y Visualizaciones":
        seccion_resultados()

@st.cache_data
def cargar_datos_diabetes():
    """Carga los datos del dataset de diabetes"""
    try:
        ds = ml_libs['fetch_ucirepo'](id=296)
        X_raw = ds.data.features.copy()
        y_raw = ds.data.targets.copy()

        # Muestra aleatoria
        sample_indices = np.random.choice(X_raw.index, size=20000, replace=False)
        X_raw_sample = X_raw.loc[sample_indices].copy()
        y_raw_sample = y_raw.loc[sample_indices].copy()

        return X_raw_sample, y_raw_sample
    except Exception as e:
        st.error(f"Error al cargar los datos: {str(e)}")
        return None, None

def seccion_carga_datos():
    st.header("üìä Carga y Exploraci√≥n de Datos")

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("Informaci√≥n del Dataset")

        if st.button("üîÑ Cargar Datos de Diabetes", type="primary"):
            with st.spinner("Cargando datos..."):
                X_raw, y_raw = cargar_datos_diabetes()

                if X_raw is not None and y_raw is not None:
                    # Guardar en session_state
                    st.session_state['X_raw'] = X_raw
                    st.session_state['y_raw'] = y_raw

                    st.success("‚úÖ Datos cargados exitosamente!")

    if 'X_raw' in st.session_state:
        X_raw = st.session_state['X_raw']
        y_raw = st.session_state['y_raw']

        with col2:
            st.markdown(
                f"""
                <div class="metric-card">
                    <h4>üìà Estad√≠sticas</h4>
                    <p><strong>Filas:</strong> {X_raw.shape[0]:,}</p>
                    <p><strong>Columnas:</strong> {X_raw.shape[1]}</p>
                    <p><strong>Tama√±o:</strong> {X_raw.memory_usage(deep=True).sum() / 1024**2:.1f} MB</p>
                </div>
                """,
                unsafe_allow_html=True
            )

        # Pesta√±as para diferentes vistas
        tab1, tab2, tab3, tab4 = st.tabs(["üìã Vista General", "üî¢ Tipos de Datos", "üéØ Variable Objetivo", "üìä Valores Faltantes"])

        with tab1:
            st.subheader("Primeras filas del dataset")
            st.dataframe(X_raw.head(10))

            st.subheader("Estad√≠sticas descriptivas")
            st.dataframe(X_raw.describe())

        with tab2:
            col1, col2 = st.columns(2)

            with col1:
                numeric_cols = X_raw.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols = X_raw.select_dtypes(include=['object', 'category']).columns.tolist()

                st.markdown(
                    f"""
                    <div class="success-card">
                        <h4>üî¢ Variables Num√©ricas ({len(numeric_cols)})</h4>
                        <p>{', '.join(numeric_cols[:5])}{'...' if len(numeric_cols) > 5 else ''}</p>
                    </div>
                    """,
                    unsafe_allow_html=True
                )

            with col2:
                st.markdown(
                    f"""
                    <div class="warning-card">
                        <h4>üìù Variables Categ√≥ricas ({len(cat_cols)})</h4>
                        <p>{', '.join(cat_cols[:5])}{'...' if len(cat_cols) > 5 else ''}</p>
                    </div>
                    """,
                    unsafe_allow_html=True
                )

            # Mostrar informaci√≥n detallada de variables categ√≥ricas
            if cat_cols:
                st.subheader("Niveles por variable categ√≥rica")
                cat_info = []
                for col in cat_cols:
                    niveles = X_raw[col].nunique()
                    cat_info.append({'Variable': col, 'Niveles': niveles})

                df_cat_info = pd.DataFrame(cat_info)
                st.dataframe(df_cat_info)

        with tab3:
            st.subheader("Distribuci√≥n de la Variable Objetivo")

            col1, col2 = st.columns(2)

            with col1:
                # Distribuci√≥n de clases
                y_counts = y_raw.iloc[:, 0].value_counts()

                fig = px.pie(
                    values=y_counts.values,
                    names=y_counts.index,
                    title="Distribuci√≥n de Clases"
                )
                st.plotly_chart(fig, use_container_width=True)

            with col2:
                # Tabla de distribuci√≥n
                st.subheader("Frecuencias")
                freq_table = pd.DataFrame({
                    'Clase': y_counts.index,
                    'Cantidad': y_counts.values,
                    'Porcentaje': (y_counts.values / y_counts.sum() * 100).round(2)
                })
                st.dataframe(freq_table)

        with tab4:
            st.subheader("An√°lisis de Valores Faltantes")

            missing_data = X_raw.isnull().sum()
            missing_percent = (missing_data / len(X_raw) * 100).round(2)

            missing_df = pd.DataFrame({
                'Variable': missing_data.index,
                'Valores Faltantes': missing_data.values,
                'Porcentaje': missing_percent.values
            })
            missing_df = missing_df[missing_df['Valores Faltantes'] > 0].sort_values('Valores Faltantes', ascending=False)

            if not missing_df.empty:
                col1, col2 = st.columns(2)

                with col1:
                    fig = px.bar(
                        missing_df.head(10),
                        x='Porcentaje',
                        y='Variable',
                        orientation='h',
                        title="Top 10 Variables con M√°s Valores Faltantes"
                    )
                    st.plotly_chart(fig, use_container_width=True)

                with col2:
                    st.dataframe(missing_df)
            else:
                st.success("‚úÖ ¬°No hay valores faltantes en el dataset!")

def seccion_preprocesamiento():
    st.header("üîß Preprocesamiento de Datos")

    if 'X_raw' not in st.session_state:
        st.warning("‚ö†Ô∏è Primero debes cargar los datos en la secci√≥n anterior.")
        return

    X_raw = st.session_state['X_raw']
    y_raw = st.session_state['y_raw']

    st.subheader("Configuraci√≥n de Preprocesamiento")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### üéõÔ∏è Par√°metros")
        thresh = st.slider("Umbral de varianza explicada (%)", 50, 99, 90) / 100

        # Selecci√≥n de variables a eliminar
        id_like_default = ["encounter_id", "patient_nbr", "encounterid", "patientnbr",
                          "diag_1", "diag_2", "diag_3", "payer_code", "medical_specialty"]

        variables_eliminar = st.multiselect(
            "Variables a eliminar (IDs y diagn√≥sticos espec√≠ficos):",
            options=X_raw.columns.tolist(),
            default=[col for col in id_like_default if col in X_raw.columns]
        )

    with col2:
        st.markdown("### ‚öôÔ∏è Par√°metros del Modelo")

        # Configuraci√≥n espec√≠fica del modelo
        if modelo_seleccionado == "Random Forest":
            n_estimators = st.slider("N√∫mero de √°rboles", 50, 500, 100)
            max_depth = st.selectbox("Profundidad m√°xima", [None, 10, 20, 30, 50])
        elif modelo_seleccionado == "Extra Trees":
            n_estimators = st.slider("N√∫mero de √°rboles", 50, 500, 100)
            max_depth = st.selectbox("Profundidad m√°xima", [None, 10, 20, 30, 50])
        elif modelo_seleccionado == "Gradient Boosting":
            max_iter = st.slider("M√°ximo de iteraciones", 50, 300, 100)
            learning_rate = st.slider("Tasa de aprendizaje", 0.01, 0.3, 0.1)
        elif modelo_seleccionado == "Regresi√≥n Log√≠stica":
            C = st.slider("Par√°metro C", 0.01, 10.0, 1.0)
            max_iter = st.slider("M√°ximo de iteraciones", 500, 3000, 1000)
        elif modelo_seleccionado == "SVM Lineal":
            C = st.slider("Par√°metro C", 0.01, 10.0, 1.0)
            max_iter = st.slider("M√°ximo de iteraciones", 500, 3000, 1000)

    # Configuraci√≥n de balanceo de clases
    st.subheader("Manejo de Desbalance de Clases")

    class_counts = Counter(y_encoded)
    total_samples = len(y_encoded)

    col1, col2 = st.columns(2)

    with col1:
        # Mostrar distribuci√≥n actual
        st.markdown("#### Distribuci√≥n Actual")
        for class_label, count in sorted(class_counts.items()):
            percentage = (count / total_samples) * 100
            st.write(f"Clase {label_encoder.classes_[class_label]}: {count} ({percentage:.1f}%)")

        # Calcular ratio de desbalance
        min_class = min(class_counts.values())
        max_class = max(class_counts.values())
        imbalance_ratio = max_class / min_class

        if imbalance_ratio > 2:
            st.warning(f"‚ö†Ô∏è Ratio de desbalance: {imbalance_ratio:.2f}:1")
        else:
            st.success(f"‚úÖ Ratio de desbalance: {imbalance_ratio:.2f}:1")

    with col2:
        balancing_method = st.selectbox(
            "M√©todo de balanceo:",
            ["Ninguno", "Class Weight Balanced", "SMOTE", "ADASYN"]
        )

        if balancing_method != "Ninguno":
            st.info(f"Se aplicar√° {balancing_method} para balancear las clases")

    if st.button("üöÄ Entrenar Modelo", type="primary"):
        with st.spinner(f"Entrenando {modelo_seleccionado}..."):
            # Preparar datos
            feature_results = st.session_state['feature_selection_results'][metodo_caracteristicas]
            selected_vars = feature_results['selected_vars']

            # Preparar dataset final
            if metodo_caracteristicas == 'chi2':
                # Solo variables categ√≥ricas seleccionadas
                X_cat_sel = st.session_state['X_cat_processed'][selected_vars]
                X_cat_sel_dum = pd.get_dummies(X_cat_sel, drop_first=True, prefix_sep='=')
                X_final = pd.concat([X_num_scaled, X_cat_sel_dum], axis=1)
            elif metodo_caracteristicas == 'anova':
                # Solo variables num√©ricas seleccionadas
                X_num_sel = X_num_scaled[selected_vars]
                X_final = pd.concat([X_num_sel, X_cat_dummies], axis=1)
            else:
                # Todas las variables (para random forest y otros m√©todos que ya seleccionaron)
                X_complete = pd.concat([X_num_scaled, X_cat_dummies.fillna(0)], axis=1)
                X_final = X_complete[selected_vars] if metodo_caracteristicas in ['random_forest'] else X_complete

            # Divisi√≥n de datos
            X_train, X_test, y_train, y_test = ml_libs['train_test_split'](
                X_final, y_encoded,
                test_size=test_size,
                stratify=y_encoded,
                random_state=RANDOM_STATE
            )

            # Crear modelo
            if modelo_seleccionado == "Random Forest":
                modelo = ml_libs['RandomForestClassifier'](
                    n_estimators=n_estimators,
                    max_depth=max_depth,
                    random_state=RANDOM_STATE
                )
            elif modelo_seleccionado == "Extra Trees":
                modelo = ml_libs['ExtraTreesClassifier'](
                    n_estimators=n_estimators,
                    max_depth=max_depth,
                    random_state=RANDOM_STATE
                )
            elif modelo_seleccionado == "Gradient Boosting":
                modelo = ml_libs['HistGradientBoostingClassifier'](
                    max_iter=max_iter,
                    learning_rate=learning_rate,
                    random_state=RANDOM_STATE
                )
            elif modelo_seleccionado == "Regresi√≥n Log√≠stica":
                modelo = ml_libs['LogisticRegression'](
                    C=C,
                    max_iter=max_iter,
                    random_state=RANDOM_STATE
                )
            elif modelo_seleccionado == "SVM Lineal":
                modelo = ml_libs['SVC'](
                    kernel='linear',
                    C=C,
                    max_iter=max_iter,
                    probability=True,
                    random_state=RANDOM_STATE
                )

            # Aplicar balanceo si es necesario
            if balancing_method == "Class Weight Balanced" and hasattr(modelo, 'class_weight'):
                modelo.set_params(class_weight='balanced')

            # Entrenar modelo
            start_time = time.time()

            if balancing_method == "SMOTE" and ml_libs['IMBALANCED_AVAILABLE']:
                try:
                    from imblearn.over_sampling import SMOTE
                    smote = SMOTE(random_state=RANDOM_STATE)
                    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
                    modelo.fit(X_train_balanced, y_train_balanced)
                    st.info("‚úÖ SMOTE aplicado exitosamente")
                except ImportError:
                    modelo.fit(X_train, y_train)
                    st.warning("‚ö†Ô∏è SMOTE no disponible, entrenando sin balanceo")
            elif balancing_method == "ADASYN" and ml_libs['IMBALANCED_AVAILABLE']:
                try:
                    from imblearn.over_sampling import ADASYN
                    adasyn = ADASYN(random_state=RANDOM_STATE)
                    X_train_balanced, y_train_balanced = adasyn.fit_resample(X_train, y_train)
                    modelo.fit(X_train_balanced, y_train_balanced)
                    st.info("‚úÖ ADASYN aplicado exitosamente")
                except ImportError:
                    modelo.fit(X_train, y_train)
                    st.warning("‚ö†Ô∏è ADASYN no disponible, entrenando sin balanceo")
            else:
                modelo.fit(X_train, y_train)

            training_time = time.time() - start_time

            # Realizar predicciones
            y_pred = modelo.predict(X_test)
            y_pred_proba = modelo.predict_proba(X_test) if hasattr(modelo, 'predict_proba') else None

            # Calcular m√©tricas
            accuracy = ml_libs['accuracy_score'](y_test, y_pred)
            f1 = ml_libs['f1_score'](y_test, y_pred, average='macro')

            # Guardar resultados
            resultado_individual = {
                'modelo': modelo,
                'nombre_modelo': modelo_seleccionado,
                'metodo_caracteristicas': metodo_caracteristicas,
                'X_train': X_train,
                'X_test': X_test,
                'y_train': y_train,
                'y_test': y_test,
                'y_pred': y_pred,
                'y_pred_proba': y_pred_proba,
                'accuracy': accuracy,
                'f1_score': f1,
                'training_time': training_time,
                'balancing_method': balancing_method,
                'selected_features': selected_vars
            }

            st.session_state['resultado_individual'] = resultado_individual

            st.success(f"‚úÖ Modelo {modelo_seleccionado} entrenado exitosamente!")

        # Mostrar m√©tricas principales
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("üéØ Precisi√≥n", f"{accuracy:.4f}")

        with col2:
            st.metric("üìä F1-Score (Macro)", f"{f1:.4f}")

        with col3:
            st.metric("‚è±Ô∏è Tiempo de Entrenamiento", f"{training_time:.2f}s")

        with col4:
            st.metric("üîß Caracter√≠sticas Usadas", len(selected_vars))

        # Visualizaciones
        st.subheader("üìà Resultados del Modelo")

        tab1, tab2, tab3 = st.tabs(["üìä Reporte de Clasificaci√≥n", "üî• Matriz de Confusi√≥n", "üìà Curva ROC"])

        with tab1:
            # Reporte de clasificaci√≥n
            class_report = ml_libs['classification_report'](
                y_test, y_pred,
                target_names=label_encoder.classes_,
                output_dict=True
            )

            # Convertir a DataFrame para mejor visualizaci√≥n
            report_df = pd.DataFrame(class_report).transpose()
            report_df = report_df.round(4)

            st.dataframe(report_df, use_container_width=True)

        with tab2:
            # Matriz de confusi√≥n
            cm = ml_libs['confusion_matrix'](y_test, y_pred)

            # Crear heatmap con plotly
            fig = px.imshow(
                cm,
                labels=dict(x="Predicci√≥n", y="Valor Real", color="Cantidad"),
                x=label_encoder.classes_,
                y=label_encoder.classes_,
                color_continuous_scale='Blues',
                text_auto=True,
                title=f"Matriz de Confusi√≥n - {modelo_seleccionado}"
            )

            st.plotly_chart(fig, use_container_width=True)

        with tab3:
            # Curva ROC (solo para clasificaci√≥n binaria)
            if len(np.unique(y_test)) == 2 and y_pred_proba is not None:
                fpr, tpr, _ = ml_libs['roc_curve'](y_test, y_pred_proba[:, 1])
                roc_auc = ml_libs['auc'](fpr, tpr)

                fig = go.Figure()

                # Curva ROC
                fig.add_trace(go.Scatter(
                    x=fpr, y=tpr,
                    mode='lines',
                    name=f'ROC Curve (AUC = {roc_auc:.3f})',
                    line=dict(color='darkorange', width=2)
                ))

                # L√≠nea diagonal
                fig.add_trace(go.Scatter(
                    x=[0, 1], y=[0, 1],
                    mode='lines',
                    name='Clasificador Aleatorio',
                    line=dict(color='navy', width=2, dash='dash')
                ))

                fig.update_layout(
                    title=f'Curva ROC - {modelo_seleccionado}',
                    xaxis_title='Tasa de Falsos Positivos',
                    yaxis_title='Tasa de Verdaderos Positivos',
                    xaxis=dict(range=[0, 1]),
                    yaxis=dict(range=[0, 1]),
                    width=600,
                    height=500
                )

                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("La curva ROC solo est√° disponible para clasificaci√≥n binaria con probabilidades.")

def seccion_comparacion_modelos():
    st.header("üèÜ Comparaci√≥n de Modelos")

    if 'feature_selection_results' not in st.session_state:
        st.warning("‚ö†Ô∏è Primero debes ejecutar la selecci√≥n de caracter√≠sticas.")
        return

    # Preparar datos
    X_num_scaled = st.session_state['X_num_scaled']
    X_cat_dummies = st.session_state['X_cat_dummies']
    y_encoded = st.session_state['y_encoded']
    label_encoder = st.session_state['label_encoder']

    st.subheader("Configuraci√≥n de la Comparaci√≥n")

    col1, col2 = st.columns(2)

    with col1:
        # Selecci√≥n de modelos a comparar
        modelos_disponibles = {
            "Random Forest": ml_libs['RandomForestClassifier'](random_state=RANDOM_STATE),
            "Extra Trees": ml_libs['ExtraTreesClassifier'](random_state=RANDOM_STATE),
            "Gradient Boosting": ml_libs['HistGradientBoostingClassifier'](random_state=RANDOM_STATE),
            "Regresi√≥n Log√≠stica": ml_libs['LogisticRegression'](random_state=RANDOM_STATE, max_iter=1000),
            "SVM Lineal": ml_libs['SVC'](kernel='linear', random_state=RANDOM_STATE, probability=True, max_iter=1000)
        }

        modelos_seleccionados = st.multiselect(
            "Selecciona modelos para comparar:",
            list(modelos_disponibles.keys()),
            default=["Random Forest", "Regresi√≥n Log√≠stica"]
        )

        # M√©todo de selecci√≥n de caracter√≠sticas
        metodo_caracteristicas = st.selectbox(
            "M√©todo de selecci√≥n de caracter√≠sticas:",
            list(st.session_state['feature_selection_results'].keys())
        )

    with col2:
        # Par√°metros de comparaci√≥n
        test_size = st.slider("Tama√±o del conjunto de prueba (%)", 10, 40, 25) / 100
        cv_folds = st.slider("N√∫mero de folds para CV", 3, 10, 5)

        # M√©todo de balanceo
        balancing_method = st.selectbox(
            "M√©todo de balanceo:",
            ["Ninguno", "Class Weight Balanced"]
        )

        # Optimizaci√≥n de hiperpar√°metros
        optimize_hyperparams = st.checkbox("Optimizar hiperpar√°metros", value=True)
        n_iter = st.slider("Iteraciones para optimizaci√≥n", 5, 50, 10) if optimize_hyperparams else 0

    if st.button("üöÄ Ejecutar Comparaci√≥n", type="primary"):
        if not modelos_seleccionados:
            st.error("‚ö†Ô∏è Selecciona al menos un modelo para comparar.")
            return

        with st.spinner("Ejecutando comparaci√≥n de modelos..."):
            # Preparar datos
            feature_results = st.session_state['feature_selection_results'][metodo_caracteristicas]
            selected_vars = feature_results['selected_vars']

            # Preparar dataset final
            if metodo_caracteristicas == 'chi2':
                X_cat_sel = st.session_state['X_cat_processed'][selected_vars]
                X_cat_sel_dum = pd.get_dummies(X_cat_sel, drop_first=True, prefix_sep='=')
                X_final = pd.concat([X_num_scaled, X_cat_sel_dum], axis=1)
            elif metodo_caracteristicas == 'anova':
                X_num_sel = X_num_scaled[selected_vars]
                X_final = pd.concat([X_num_sel, X_cat_dummies], axis=1)
            else:
                X_complete = pd.concat([X_num_scaled, X_cat_dummies.fillna(0)], axis=1)
                X_final = X_complete[selected_vars] if metodo_caracteristicas in ['random_forest'] else X_complete

            # Divisi√≥n de datos
            X_train, X_test, y_train, y_test = ml_libs['train_test_split'](
                X_final, y_encoded,
                test_size=test_size,
                stratify=y_encoded,
                random_state=RANDOM_STATE
            )

            # Resultados de comparaci√≥n
            resultados_comparacion = {}

            # Definir espacios de par√°metros para optimizaci√≥n
            param_grids = {
                "Random Forest": {
                    'n_estimators': ml_libs['randint'](50, 200),
                    'max_depth': [10, 20, 30, None],
                    'min_samples_split': ml_libs['randint'](2, 10)
                },
                "Extra Trees": {
                    'n_estimators': ml_libs['randint'](50, 200),
                    'max_depth': [10, 20, 30, None],
                    'min_samples_split': ml_libs['randint'](2, 10)
                },
                "Gradient Boosting": {
                    'max_iter': ml_libs['randint'](50, 200),
                    'learning_rate': ml_libs['uniform'](0.05, 0.25),
                    'max_depth': ml_libs['randint'](3, 10)
                },
                "Regresi√≥n Log√≠stica": {
                    'C': ml_libs['uniform'](0.1, 10),
                    'max_iter': [1000, 2000]
                },
                "SVM Lineal": {
                    'C': ml_libs['uniform'](0.1, 10),
                    'max_iter': [1000, 2000]
                }
            }

            # Entrenar cada modelo
            progress_bar = st.progress(0)
            status_text = st.empty()

            for i, nombre_modelo in enumerate(modelos_seleccionados):
                status_text.text(f"Entrenando {nombre_modelo}...")
                progress_bar.progress((i + 1) / len(modelos_seleccionados))

                try:
                    start_time = time.time()

                    # Obtener modelo base
                    modelo_base = modelos_disponibles[nombre_modelo]

                    # Aplicar balanceo si es necesario
                    if balancing_method == "Class Weight Balanced" and hasattr(modelo_base, 'class_weight'):
                        modelo_base.set_params(class_weight='balanced')

                    # Optimizaci√≥n de hiperpar√°metros
                    if optimize_hyperparams and nombre_modelo in param_grids:
                        random_search = ml_libs['RandomizedSearchCV'](
                            modelo_base,
                            param_distributions=param_grids[nombre_modelo],
                            n_iter=n_iter,
                            cv=cv_folds,
                            scoring='f1_macro',
                            random_state=RANDOM_STATE,
                            n_jobs=-1
                        )

                        random_search.fit(X_train, y_train)
                        best_model = random_search.best_estimator_
                        cv_score = random_search.best_score_
                        best_params = random_search.best_params_
                    else:
                        modelo_base.fit(X_train, y_train)
                        best_model = modelo_base
                        cv_score = None
                        best_params = {}

                    # Predicciones
                    y_pred = best_model.predict(X_test)
                    y_pred_proba = best_model.predict_proba(X_test) if hasattr(best_model, 'predict_proba') else None

                    # M√©tricas
                    accuracy = ml_libs['accuracy_score'](y_test, y_pred)
                    f1 = ml_libs['f1_score'](y_test, y_pred, average='macro')

                    training_time = time.time() - start_time

                    # Guardar resultados
                    resultados_comparacion[nombre_modelo] = {
                        'model': best_model,
                        'cv_score': cv_score,
                        'test_accuracy': accuracy,
                        'test_f1': f1,
                        'y_pred': y_pred,
                        'y_pred_proba': y_pred_proba,
                        'training_time': training_time,
                        'best_params': best_params
                    }

                except Exception as e:
                    st.error(f"Error entrenando {nombre_modelo}: {str(e)}")
                    resultados_comparacion[nombre_modelo] = {
                        'error': str(e),
                        'training_time': time.time() - start_time
                    }

            progress_bar.empty()
            status_text.empty()

            # Guardar en session_state
            st.session_state['resultados_comparacion'] = {
                'results': resultados_comparacion,
                'X_train': X_train,
                'X_test': X_test,
                'y_train': y_train,
                'y_test': y_test,
                'selected_features': selected_vars,
                'feature_method': metodo_caracteristicas
            }

            st.success(f"‚úÖ Comparaci√≥n completada! {len(modelos_seleccionados)} modelos entrenados.")

        # Mostrar resultados
        mostrar_resultados_comparacion()

def mostrar_resultados_comparacion():
    """Muestra los resultados de la comparaci√≥n de modelos"""
    if 'resultados_comparacion' not in st.session_state:
        return

    resultados = st.session_state['resultados_comparacion']['results']
    y_test = st.session_state['resultados_comparacion']['y_test']
    label_encoder = st.session_state['label_encoder']

    st.subheader("üìä Resultados de la Comparaci√≥n")

    # Tabla comparativa
    comparison_data = []
    successful_models = {}

    for nombre, resultado in resultados.items():
        if 'error' not in resultado:
            comparison_data.append({
                'Modelo': nombre,
                'CV F1-Score': f"{resultado['cv_score']:.4f}" if resultado['cv_score'] else "N/A",
                'Test Accuracy': f"{resultado['test_accuracy']:.4f}",
                'Test F1-Score': f"{resultado['test_f1']:.4f}",
                'Tiempo (s)': f"{resultado['training_time']:.2f}"
            })
            successful_models[nombre] = resultado
        else:
            comparison_data.append({
                'Modelo': nombre,
                'CV F1-Score': "ERROR",
                'Test Accuracy': "ERROR",
                'Test F1-Score': "ERROR",
                'Tiempo (s)': f"{resultado['training_time']:.2f}"
            })

    comparison_df = pd.DataFrame(comparison_data)
    st.dataframe(comparison_df, use_container_width=True)

    if successful_models:
        # Identificar el mejor modelo
        best_model_name = max(successful_models.items(), key=lambda x: x[1]['test_f1'])[0]

        st.markdown(
            f"""
            <div class="success-card">
                <h4>üèÜ Mejor Modelo</h4>
                <p><strong>{best_model_name}</strong></p>
                <p>F1-Score: {successful_models[best_model_name]['test_f1']:.4f}</p>
            </div>
            """,
            unsafe_allow_html=True
        )

        # Visualizaciones comparativas
        st.subheader("üìä Visualizaciones Comparativas")

        tab1, tab2, tab3 = st.tabs(["üìà M√©tricas", "‚è±Ô∏è Tiempos", "üîÑ Matrices de Confusi√≥n"])

        with tab1:
            # Gr√°fico de barras con m√©tricas
            metrics_data = []
            for nombre, resultado in successful_models.items():
                metrics_data.extend([
                    {'Modelo': nombre, 'M√©trica': 'Accuracy', 'Valor': resultado['test_accuracy']},
                    {'Modelo': nombre, 'M√©trica': 'F1-Score', 'Valor': resultado['test_f1']}
                ])

            metrics_df = pd.DataFrame(metrics_data)

            fig = px.bar(
                metrics_df,
                x='Modelo',
                y='Valor',
                color='M√©trica',
                title="Comparaci√≥n de M√©tricas de Rendimiento",
                barmode='group'
            )
            fig.update_layout(yaxis_range=[0, 1])
            st.plotly_chart(fig, use_container_width=True)

        with tab2:
            # Gr√°fico de tiempos de entrenamiento
            tiempo_data = [(nombre, resultado['training_time'])
                          for nombre, resultado in successful_models.items()]

            tiempo_df = pd.DataFrame(tiempo_data, columns=['Modelo', 'Tiempo'])

            fig = px.bar(
                tiempo_df,
                x='Modelo',
                y='Tiempo',
                title="Tiempos de Entrenamiento (segundos)"
            )
            st.plotly_chart(fig, use_container_width=True)

        with tab3:
            # Matrices de confusi√≥n para cada modelo
            for nombre, resultado in successful_models.items():
                if 'y_pred' in resultado:
                    cm = ml_libs['confusion_matrix'](y_test, resultado['y_pred'])

                    fig = px.imshow(
                        cm,
                        labels=dict(x="Predicci√≥n", y="Valor Real", color="Cantidad"),
                        x=label_encoder.classes_,
                        y=label_encoder.classes_,
                        color_continuous_scale='Blues',
                        text_auto=True,
                        title=f"Matriz de Confusi√≥n - {nombre}"
                    )

                    st.plotly_chart(fig, use_container_width=True)

def seccion_resultados():
    st.header("üìà Resultados y Visualizaciones")

    # Verificar si hay resultados disponibles
    has_individual = 'resultado_individual' in st.session_state
    has_comparison = 'resultados_comparacion' in st.session_state

    if not has_individual and not has_comparison:
        st.warning("‚ö†Ô∏è No hay resultados disponibles. Ejecuta primero el modelado individual o la comparaci√≥n de modelos.")
        return

    # Selector de tipo de resultado
    result_type = st.selectbox(
        "Selecciona el tipo de resultado a visualizar:",
        ["Modelo Individual", "Comparaci√≥n de Modelos"] if has_individual and has_comparison
        else (["Modelo Individual"] if has_individual else ["Comparaci√≥n de Modelos"])
    )

    if result_type == "Modelo Individual" and has_individual:
        mostrar_resultados_individuales()
    elif result_type == "Comparaci√≥n de Modelos" and has_comparison:
        mostrar_visualizaciones_comparacion()

def mostrar_resultados_individuales():
    """Muestra resultados detallados del modelo individual"""
    resultado = st.session_state['resultado_individual']
    label_encoder = st.session_state['label_encoder']

    st.subheader(f"üìä Resultados Detallados - {resultado['nombre_modelo']}")

    # M√©tricas principales
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("üéØ Precisi√≥n", f"{resultado['accuracy']:.4f}")

    with col2:
        st.metric("üìä F1-Score", f"{resultado['f1_score']:.4f}")

    with col3:
        st.metric("‚è±Ô∏è Tiempo", f"{resultado['training_time']:.2f}s")

    with col4:
        st.metric("üîß Caracter√≠sticas", len(resultado['selected_features']))

    # Informaci√≥n del modelo
    st.markdown(
        f"""
        <div class="metric-card">
            <h4>‚ÑπÔ∏è Informaci√≥n del Modelo</h4>
            <p><strong>Modelo:</strong> {resultado['nombre_modelo']}</p>
            <p><strong>M√©todo de selecci√≥n:</strong> {resultado['metodo_caracteristicas']}</p>
            <p><strong>Balanceo:</strong> {resultado['balancing_method']}</p>
            <p><strong>Tama√±o entrenamiento:</strong> {resultado['X_train'].shape[0]:,} muestras</p>
            <p><strong>Tama√±o prueba:</strong> {resultado['X_test'].shape[0]:,} muestras</p>
        </div>
        """,
        unsafe_allow_html=True
    )

    # Visualizaciones detalladas
    tab1, tab2, tab3, tab4 = st.tabs(["üìä M√©tricas Detalladas", "üî• Matriz de Confusi√≥n", "üìà An√°lisis de Caracter√≠sticas", "üéØ Predicciones"])

    with tab1:
        # Reporte de clasificaci√≥n detallado
        y_test = resultado['y_test']
        y_pred = resultado['y_pred']

        class_report = ml_libs['classification_report'](
            y_test, y_pred,
            target_names=label_encoder.classes_,
            output_dict=True
        )

        # Convertir a DataFrame
        report_df = pd.DataFrame(class_report).transpose()
        report_df = report_df.round(4)

        st.subheader("Reporte de Clasificaci√≥n Completo")
        st.dataframe(report_df, use_container_width=True)

        # M√©tricas por clase en gr√°fico
        class_metrics = report_df.drop(['accuracy', 'macro avg', 'weighted avg'], errors='ignore')

        if not class_metrics.empty:
            fig = go.Figure()

            for metric in ['precision', 'recall', 'f1-score']:
                if metric in class_metrics.columns:
                    fig.add_trace(go.Bar(
                        name=metric.capitalize(),
                        x=class_metrics.index,
                        y=class_metrics[metric],
                        text=class_metrics[metric].round(3),
                        textposition='auto',
                    ))

            fig.update_layout(
                title='M√©tricas por Clase',
                xaxis_title='Clases',
                yaxis_title='Valor',
                barmode='group',
                yaxis_range=[0, 1]
            )

            st.plotly_chart(fig, use_container_width=True)

    with tab2:
        # Matriz de confusi√≥n interactiva
        cm = ml_libs['confusion_matrix'](y_test, y_pred)

        # Matriz de confusi√≥n normalizada
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

        col1, col2 = st.columns(2)

        with col1:
            st.subheader("Matriz de Confusi√≥n (Absoluta)")
            fig1 = px.imshow(
                cm,
                labels=dict(x="Predicci√≥n", y="Valor Real", color="Cantidad"),
                x=label_encoder.classes_,
                y=label_encoder.classes_,
                color_continuous_scale='Blues',
                text_auto=True,
                aspect="auto"
            )
            st.plotly_chart(fig1, use_container_width=True)

        with col2:
            st.subheader("Matriz de Confusi√≥n (Normalizada)")
            fig2 = px.imshow(
                cm_normalized,
                labels=dict(x="Predicci√≥n", y="Valor Real", color="Proporci√≥n"),
                x=label_encoder.classes_,
                y=label_encoder.classes_,
                color_continuous_scale='Blues',
                text_auto='.3f',
                aspect="auto"
            )
            st.plotly_chart(fig2, use_container_width=True)

        # An√°lisis de errores
        st.subheader("An√°lisis de Errores")

        # Calcular errores por clase
        errors_data = []
        for i, clase_real in enumerate(label_encoder.classes_):
            for j, clase_pred in enumerate(label_encoder.classes_):
                if i != j and cm[i, j] > 0:  # Solo errores
                    errors_data.append({
                        'Clase Real': clase_real,
                        'Predicci√≥n Incorrecta': clase_pred,
                        'Cantidad': cm[i, j],
                        'Porcentaje': f"{cm_normalized[i, j]:.1%}"
                    })

        if errors_data:
            errors_df = pd.DataFrame(errors_data).sort_values('Cantidad', ascending=False)
            st.dataframe(errors_df, use_container_width=True)
        else:
            st.success("¬°No hay errores de clasificaci√≥n!")

    with tab3:
        # An√°lisis de importancia de caracter√≠sticas (si disponible)
        modelo = resultado['modelo']

        if hasattr(modelo, 'feature_importances_'):
            st.subheader("Importancia de Caracter√≠sticas")

            importances = modelo.feature_importances_
            feature_names = resultado['X_train'].columns

            # Crear DataFrame de importancias
            importance_df = pd.DataFrame({
                'Caracter√≠stica': feature_names,
                'Importancia': importances
            }).sort_values('Importancia', ascending=False)

            # Top 20 caracter√≠sticas
            top_features = importance_df.head(20)

            col1, col2 = st.columns(2)

            with col1:
                fig = px.bar(
                    top_features,
                    x='Importancia',
                    y='Caracter√≠stica',
                    orientation='h',
                    title='Top 20 Caracter√≠sticas M√°s Importantes'
                )
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, use_container_width=True)

            with col2:
                st.subheader("Tabla de Importancias")
                st.dataframe(
                    top_features.reset_index(drop=True),
                    use_container_width=True
                )

        elif hasattr(modelo, 'coef_'):
            st.subheader("Coeficientes del Modelo")

            coefs = modelo.coef_[0] if len(modelo.coef_.shape) > 1 else modelo.coef_
            feature_names = resultado['X_train'].columns

            # Crear DataFrame de coeficientes
            coef_df = pd.DataFrame({
                'Caracter√≠stica': feature_names,
                'Coeficiente': coefs,
                'Importancia_Abs': np.abs(coefs)
            }).sort_values('Importancia_Abs', ascending=False)

            # Top 20 caracter√≠sticas
            top_coefs = coef_df.head(20)

            col1, col2 = st.columns(2)

            with col1:
                fig = px.bar(
                    top_coefs,
                    x='Coeficiente',
                    y='Caracter√≠stica',
                    orientation='h',
                    title='Top 20 Coeficientes M√°s Importantes',
                    color='Coeficiente',
                    color_continuous_scale='RdBu'
                )
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, use_container_width=True)

            with col2:
                st.subheader("Tabla de Coeficientes")
                st.dataframe(
                    top_coefs[['Caracter√≠stica', 'Coeficiente']].reset_index(drop=True),
                    use_container_width=True
                )

        else:
            st.info("Este modelo no proporciona informaci√≥n sobre la importancia de caracter√≠sticas.")

        # Informaci√≥n sobre caracter√≠sticas seleccionadas
        st.subheader("Caracter√≠sticas Seleccionadas")

        selected_features = resultado['selected_features']

        col1, col2 = st.columns(2)

        with col1:
            st.markdown(
                f"""
                <div class="metric-card">
                    <h4>üìä Estad√≠sticas de Selecci√≥n</h4>
                    <p><strong>Total seleccionadas:</strong> {len(selected_features)}</p>
                    <p><strong>M√©todo usado:</strong> {resultado['metodo_caracteristicas']}</p>
                </div>
                """,
                unsafe_allow_html=True
            )

        with col2:
            # Mostrar algunas caracter√≠sticas seleccionadas
            st.subheader("Muestra de Caracter√≠sticas")
            sample_features = selected_features[:10] if len(selected_features) > 10 else selected_features

            for i, feature in enumerate(sample_features, 1):
                st.write(f"{i}. {feature}")

            if len(selected_features) > 10:
                st.write(f"... y {len(selected_features) - 10} m√°s")

    with tab4:
        # An√°lisis de predicciones
        st.subheader("An√°lisis de Predicciones")

        # Distribuci√≥n de predicciones vs real
        col1, col2 = st.columns(2)

        with col1:
            # Distribuci√≥n real
            real_counts = pd.Series(y_test).map(lambda x: label_encoder.classes_[x]).value_counts()

            fig1 = px.pie(
                values=real_counts.values,
                names=real_counts.index,
                title="Distribuci√≥n Real (Test)"
            )
            st.plotly_chart(fig1, use_container_width=True)

        with col2:
            # Distribuci√≥n predicha
            pred_counts = pd.Series(y_pred).map(lambda x: label_encoder.classes_[x]).value_counts()

            fig2 = px.pie(
                values=pred_counts.values,
                names=pred_counts.index,
                title="Distribuci√≥n Predicha"
            )
            st.plotly_chart(fig2, use_container_width=True)

        # Tabla de comparaci√≥n
        st.subheader("Comparaci√≥n de Distribuciones")

        comparison_dist = pd.DataFrame({
            'Clase': label_encoder.classes_,
            'Real': [real_counts.get(clase, 0) for clase in label_encoder.classes_],
            'Predicho': [pred_counts.get(clase, 0) for clase in label_encoder.classes_]
        })

        comparison_dist['Diferencia'] = comparison_dist['Predicho'] - comparison_dist['Real']
        comparison_dist['% Real'] = (comparison_dist['Real'] / comparison_dist['Real'].sum() * 100).round(1)
        comparison_dist['% Predicho'] = (comparison_dist['Predicho'] / comparison_dist['Predicho'].sum() * 100).round(1)

        st.dataframe(comparison_dist, use_container_width=True)

        # Probabilidades de predicci√≥n (si disponible)
        if resultado['y_pred_proba'] is not None:
            st.subheader("Distribuci√≥n de Probabilidades")

            y_pred_proba = resultado['y_pred_proba']

            # Histograma de probabilidades m√°ximas
            max_probs = np.max(y_pred_proba, axis=1)

            fig = px.histogram(
                x=max_probs,
                nbins=20,
                title="Distribuci√≥n de Probabilidades M√°ximas",
                labels={'x': 'Probabilidad M√°xima', 'y': 'Frecuencia'}
            )

            st.plotly_chart(fig, use_container_width=True)

            # Estad√≠sticas de confianza
            confidence_stats = {
                'Probabilidad media': f"{np.mean(max_probs):.4f}",
                'Probabilidad m√≠nima': f"{np.min(max_probs):.4f}",
                'Probabilidad m√°xima': f"{np.max(max_probs):.4f}",
                'Desviaci√≥n est√°ndar': f"{np.std(max_probs):.4f}"
            }

            for metric, value in confidence_stats.items():
                st.metric(metric, value)

def mostrar_visualizaciones_comparacion():
    """Muestra visualizaciones avanzadas de la comparaci√≥n"""
    if 'resultados_comparacion' not in st.session_state:
        return

    resultados = st.session_state['resultados_comparacion']['results']
    y_test = st.session_state['resultados_comparacion']['y_test']
    label_encoder = st.session_state['label_encoder']

    st.subheader("üìä Visualizaciones Avanzadas de Comparaci√≥n")

    # Filtrar solo modelos exitosos
    successful_models = {k: v for k, v in resultados.items() if 'error' not in v}

    if not successful_models:
        st.error("No hay modelos exitosos para visualizar.")
        return

    tab1, tab2, tab3, tab4 = st.tabs(["üèÜ Ranking de Modelos", "üìà An√°lisis de Rendimiento", "‚öñÔ∏è Trade-offs", "üîç An√°lisis Detallado"])

    with tab1:
        st.subheader("Ranking de Modelos")

        # Crear ranking basado en m√∫ltiples m√©tricas
        ranking_data = []
        for nombre, resultado in successful_models.items():
            ranking_data.append({
                'Modelo': nombre,
                'Accuracy': resultado['test_accuracy'],
                'F1-Score': resultado['test_f1'],
                'Tiempo': resultado['training_time'],
                'CV_Score': resultado['cv_score'] if resultado['cv_score'] else 0
            })

        ranking_df = pd.DataFrame(ranking_data)

        # Normalizar m√©tricas para ranking (tiempo invertido)
        ranking_df['Accuracy_norm'] = ranking_df['Accuracy'] / ranking_df['Accuracy'].max()
        ranking_df['F1_norm'] = ranking_df['F1-Score'] / ranking_df['F1-Score'].max()
        ranking_df['Tiempo_norm'] = ranking_df['Tiempo'].min() / ranking_df['Tiempo']  # Invertir tiempo
        ranking_df['CV_norm'] = ranking_df['CV_Score'] / ranking_df['CV_Score'].max() if ranking_df['CV_Score'].max() > 0 else 0

        # Score compuesto
        ranking_df['Score_Compuesto'] = (
            ranking_df['Accuracy_norm'] * 0.3 +
            ranking_df['F1_norm'] * 0.4 +
            ranking_df['Tiempo_norm'] * 0.2 +
            ranking_df['CV_norm'] * 0.1
        )

        ranking_df = ranking_df.sort_values('Score_Compuesto', ascending=False)

        # Gr√°fico de ranking
        fig = px.bar(
            ranking_df,
            x='Modelo',
            y='Score_Compuesto',
            title='Ranking Compuesto de Modelos',
            color='Score_Compuesto',
            color_continuous_scale='viridis'
        )
        st.plotly_chart(fig, use_container_width=True)

        # Tabla de ranking
        display_cols = ['Modelo', 'Accuracy', 'F1-Score', 'Tiempo', 'Score_Compuesto']
        st.dataframe(ranking_df[display_cols].round(4), use_container_width=True)

    with tab2:
        st.subheader("An√°lisis de Rendimiento")

        # Radar chart de m√©tricas
        metrics_for_radar = []
        models_names = []

        for nombre, resultado in successful_models.items():
            models_names.append(nombre)
            metrics_for_radar.append([
                resultado['test_accuracy'],
                resultado['test_f1'],
                1 / (resultado['training_time'] + 0.1),  # Invertir tiempo
                resultado['cv_score'] if resultado['cv_score'] else 0
            ])

        # Normalizar m√©tricas para radar chart
        metrics_array = np.array(metrics_for_radar)
        metrics_normalized = metrics_array / metrics_array.max(axis=0)

        # Crear radar chart
        categories = ['Accuracy', 'F1-Score', 'Velocidad', 'CV Score']

        fig = go.Figure()

        colors = px.colors.qualitative.Set1[:len(models_names)]

        for i, (nombre, metrics) in enumerate(zip(models_names, metrics_normalized)):
            fig.add_trace(go.Scatterpolar(
                r=metrics.tolist() + [metrics[0]],  # Cerrar el pol√≠gono
                theta=categories + [categories[0]],
                fill='toself',
                name=nombre,
                line_color=colors[i]
            ))

        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )),
            showlegend=True,
            title="Comparaci√≥n de Rendimiento (Normalizado)"
        )

        st.plotly_chart(fig, use_container_width=True)

        # Box plots de m√©tricas
        col1, col2 = st.columns(2)

        with col1:
            # Distribuci√≥n de accuracy
            accuracy_data = [{'Modelo': k, 'Accuracy': v['test_accuracy']} for k, v in successful_models.items()]
            accuracy_df = pd.DataFrame(accuracy_data)

            fig = px.box(accuracy_df, y='Accuracy', title='Distribuci√≥n de Accuracy')
            fig.add_scatter(x=accuracy_df['Modelo'], y=accuracy_df['Accuracy'], mode='markers', name='Modelos')
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            # Distribuci√≥n de F1-Score
            f1_data = [{'Modelo': k, 'F1-Score': v['test_f1']} for k, v in successful_models.items()]
            f1_df = pd.DataFrame(f1_data)

            fig = px.box(f1_df, y='F1-Score', title='Distribuci√≥n de F1-Score')
            fig.add_scatter(x=f1_df['Modelo'], y=f1_df['F1-Score'], mode='markers', name='Modelos')
            st.plotly_chart(fig, use_container_width=True)

    with tab3:
        st.subheader("An√°lisis de Trade-offs")

        # Scatter plot: Accuracy vs Tiempo
        col1, col2 = st.columns(2)

        with col1:
            scatter_data = []
            for nombre, resultado in successful_models.items():
                scatter_data.append({
                    'Modelo': nombre,
                    'Accuracy': resultado['test_accuracy'],
                    'Tiempo': resultado['training_time'],
                    'F1-Score': resultado['test_f1']
                })

            scatter_df = pd.DataFrame(scatter_data)

            fig = px.scatter(
                scatter_df,
                x='Tiempo',
                y='Accuracy',
                color='Modelo',
                size='F1-Score',
                title='Trade-off: Accuracy vs Tiempo de Entrenamiento',
                labels={'Tiempo': 'Tiempo de Entrenamiento (s)'}
            )
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            # F1-Score vs Tiempo
            fig = px.scatter(
                scatter_df,
                x='Tiempo',
                y='F1-Score',
                color='Modelo',
                size='Accuracy',
                title='Trade-off: F1-Score vs Tiempo de Entrenamiento',
                labels={'Tiempo': 'Tiempo de Entrenamiento (s)'}
            )
            st.plotly_chart(fig, use_container_width=True)

        # An√°lisis de eficiencia
        st.subheader("An√°lisis de Eficiencia")

        # Calcular eficiencia (rendimiento / tiempo)
        efficiency_data = []
        for nombre, resultado in successful_models.items():
            efficiency = resultado['test_f1'] / resultado['training_time']
            efficiency_data.append({
                'Modelo': nombre,
                'Eficiencia': efficiency,
                'F1-Score': resultado['test_f1'],
                'Tiempo': resultado['training_time']
            })

        efficiency_df = pd.DataFrame(efficiency_data).sort_values('Eficiencia', ascending=False)

        fig = px.bar(
            efficiency_df,
            x='Modelo',
            y='Eficiencia',
            title='Eficiencia de Modelos (F1-Score / Tiempo)',
            color='Eficiencia',
            color_continuous_scale='viridis'
        )
        st.plotly_chart(fig, use_container_width=True)

        st.dataframe(efficiency_df.round(4), use_container_width=True)

    with tab4:
        st.subheader("An√°lisis Detallado por Modelo")

        # Selector de modelo para an√°lisis detallado
        modelo_seleccionado = st.selectbox(
            "Selecciona un modelo para an√°lisis detallado:",
            list(successful_models.keys())
        )

        if modelo_seleccionado:
            resultado = successful_models[modelo_seleccionado]

            # Informaci√≥n del modelo seleccionado
            col1, col2, col3 = st.columns(3)

            with col1:
                st.metric("üéØ Accuracy", f"{resultado['test_accuracy']:.4f}")
                st.metric("üìä F1-Score", f"{resultado['test_f1']:.4f}")

            with col2:
                st.metric("‚è±Ô∏è Tiempo", f"{resultado['training_time']:.2f}s")
                if resultado['cv_score']:
                    st.metric("üîÑ CV Score", f"{resultado['cv_score']:.4f}")

            with col3:
                if resultado['best_params']:
                    st.markdown("**üîß Mejores Par√°metros:**")
                    for param, value in resultado['best_params'].items():
                        st.write(f"‚Ä¢ {param}: {value}")

            # Matriz de confusi√≥n del modelo seleccionado
            if 'y_pred' in resultado:
                st.subheader(f"Matriz de Confusi√≥n - {modelo_seleccionado}")

                cm = ml_libs['confusion_matrix'](y_test, resultado['y_pred'])

                fig = px.imshow(
                    cm,
                    labels=dict(x="Predicci√≥n", y="Valor Real", color="Cantidad"),
                    x=label_encoder.classes_,
                    y=label_encoder.classes_,
                    color_continuous_scale='Blues',
                    text_auto=True,
                    title=f"Matriz de Confusi√≥n - {modelo_seleccionado}"
                )

                st.plotly_chart(fig, use_container_width=True)

                # Reporte de clasificaci√≥n
                class_report = ml_libs['classification_report'](
                    y_test, resultado['y_pred'],
                    target_names=label_encoder.classes_,
                    output_dict=True
                )

                report_df = pd.DataFrame(class_report).transpose().round(4)
                st.dataframe(report_df, use_container_width=True)

# Funci√≥n principal para ejecutar la aplicaci√≥n
if __name__ == "__main__":
    main()
     ‚ÑπÔ∏è Informaci√≥n")
        st.info("""
        **Pasos del preprocesamiento:**
        1. Eliminaci√≥n de variables ID y diagn√≥sticos espec√≠ficos
        2. Separaci√≥n de variables num√©ricas y categ√≥ricas
        3. Imputaci√≥n de valores faltantes
        4. Codificaci√≥n de variables categ√≥ricas
        5. Escalado de variables num√©ricas
        """)

    if st.button("üöÄ Ejecutar Preprocesamiento", type="primary"):
        with st.spinner("Ejecutando preprocesamiento..."):
            # Eliminar variables seleccionadas
            X_processed = X_raw.drop(columns=variables_eliminar, errors="ignore")

            # Separar variables num√©ricas y categ√≥ricas
            numeric_cols = X_processed.select_dtypes(include=[np.number]).columns.tolist()
            cat_cols = X_processed.select_dtypes(include=['object', 'category']).columns.tolist()

            X_num = X_processed[numeric_cols].copy()
            X_cat = X_processed[cat_cols].copy()

            # Procesar variables categ√≥ricas
            X_cat_filled = X_cat.fillna('Missing').astype(str)
            X_cat_dummies = pd.get_dummies(X_cat_filled, drop_first=False, prefix_sep='=')

            # Procesar variables num√©ricas
            X_num_processed = X_num.copy()
            for c in X_num_processed.columns:
                if not np.issubdtype(X_num_processed[c].dtype, np.number):
                    X_num_processed[c] = pd.to_numeric(X_num_processed[c], errors='coerce')

            X_num_processed = X_num_processed.fillna(X_num_processed.median(numeric_only=True))

            # Escalar variables num√©ricas
            scaler = ml_libs['StandardScaler']()
            X_num_scaled = pd.DataFrame(
                scaler.fit_transform(X_num_processed),
                columns=X_num_processed.columns,
                index=X_num_processed.index
            )

            # Codificar variable objetivo
            label_encoder = ml_libs['LabelEncoder']()
            y_encoded = label_encoder.fit_transform(y_raw.astype(str).iloc[:, 0])

            # Guardar en session_state
            st.session_state['X_num_processed'] = X_num_processed
            st.session_state['X_num_scaled'] = X_num_scaled
            st.session_state['X_cat_processed'] = X_cat_filled
            st.session_state['X_cat_dummies'] = X_cat_dummies
            st.session_state['y_encoded'] = y_encoded
            st.session_state['label_encoder'] = label_encoder
            st.session_state['scaler'] = scaler
            st.session_state['thresh'] = thresh

            st.success("‚úÖ Preprocesamiento completado!")

        # Mostrar resultados
        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown(
                f"""
                <div class="metric-card">
                    <h4>üî¢ Variables Num√©ricas</h4>
                    <p><strong>Original:</strong> {len(numeric_cols)}</p>
                    <p><strong>Procesadas:</strong> {X_num_scaled.shape[1]}</p>
                </div>
                """,
                unsafe_allow_html=True
            )

        with col2:
            st.markdown(
                f"""
                <div class="metric-card">
                    <h4>üìù Variables Categ√≥ricas</h4>
                    <p><strong>Original:</strong> {len(cat_cols)}</p>
                    <p><strong>Dummies:</strong> {X_cat_dummies.shape[1]}</p>
                </div>
                """,
                unsafe_allow_html=True
            )

        with col3:
            st.markdown(
                f"""
                <div class="metric-card">
                    <h4>üóëÔ∏è Variables Eliminadas</h4>
                    <p><strong>Cantidad:</strong> {len(variables_eliminar)}</p>
                </div>
                """,
                unsafe_allow_html=True
            )

        # Mostrar distribuci√≥n de la variable objetivo
        st.subheader("Distribuci√≥n de la Variable Objetivo Codificada")

        col1, col2 = st.columns(2)

        with col1:
            class_counts = Counter(y_encoded)
            labels = [label_encoder.classes_[i] for i in class_counts.keys()]
            values = list(class_counts.values())

            fig = px.bar(x=labels, y=values, title="Distribuci√≥n de Clases Codificadas")
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            # Tabla de mapeo
            mapping_df = pd.DataFrame({
                'Clase Original': label_encoder.classes_,
                'C√≥digo': range(len(label_encoder.classes_)),
                'Cantidad': [class_counts[i] for i in range(len(label_encoder.classes_))]
            })
            st.dataframe(mapping_df)

def seccion_seleccion_caracteristicas():
    st.header("üìâ Selecci√≥n de Caracter√≠sticas")

    if 'X_num_scaled' not in st.session_state:
        st.warning("‚ö†Ô∏è Primero debes ejecutar el preprocesamiento.")
        return

    X_num_scaled = st.session_state['X_num_scaled']
    X_cat_dummies = st.session_state['X_cat_dummies']
    y_encoded = st.session_state['y_encoded']
    thresh = st.session_state['thresh']

    st.subheader("M√©todos de Selecci√≥n de Caracter√≠sticas")

    # M√©todos disponibles
    metodos = st.multiselect(
        "Selecciona los m√©todos a aplicar:",
        ["Chi-cuadrado (Categ√≥ricas)", "ANOVA F (Num√©ricas)", "Random Forest (Embedded)", "RFECV (Wrapper)"],
        default=["Chi-cuadrado (Categ√≥ricas)", "ANOVA F (Num√©ricas)"]
    )

    if st.button("üîç Ejecutar Selecci√≥n de Caracter√≠sticas", type="primary"):
        results = {}

        with st.spinner("Ejecutando selecci√≥n de caracter√≠sticas..."):

            # M√©todo 1: Chi-cuadrado para categ√≥ricas
            if "Chi-cuadrado (Categ√≥ricas)" in metodos:
                st.subheader("1Ô∏è‚É£ Chi-cuadrado para Variables Categ√≥ricas")

                chi2_scores, _ = ml_libs['chi2'](X_cat_dummies.fillna(0), y_encoded)

                # Agrupar scores por variable original
                var_of_dummy = X_cat_dummies.columns.to_series().str.split('=', n=1, expand=True)[0]
                var_importance_cat = (
                    pd.DataFrame({'var': var_of_dummy, 'score': chi2_scores})
                    .groupby('var', sort=False)['score']
                    .sum()
                    .sort_values(ascending=False)
                )

                cum_cat = var_importance_cat.cumsum() / var_importance_cat.sum()
                cat_vars_sel = cum_cat[cum_cat <= thresh].index.tolist()
                if len(cat_vars_sel) < len(var_importance_cat):
                    cat_vars_sel = cat_vars_sel + [var_importance_cat.index[len(cat_vars_sel)]]

                results['chi2'] = {
                    'scores': var_importance_cat,
                    'selected_vars': cat_vars_sel,
                    'method': 'Chi-cuadrado'
                }

                # Visualizaci√≥n
                col1, col2 = st.columns(2)

                with col1:
                    fig = px.bar(
                        x=var_importance_cat.head(10).index,
                        y=var_importance_cat.head(10).values,
                        title="Top 10 Variables Categ√≥ricas (Chi¬≤)"
                    )
                    fig.update_xaxis(tickangle=45)
                    st.plotly_chart(fig, use_container_width=True)

                with col2:
                    st.markdown(
                        f"""
                        <div class="success-card">
                            <h4>üìä Resultados Chi¬≤</h4>
                            <p><strong>Variables disponibles:</strong> {len(var_importance_cat)}</p>
                            <p><strong>Variables seleccionadas:</strong> {len(cat_vars_sel)}</p>
                            <p><strong>Umbral:</strong> {thresh:.1%}</p>
                        </div>
                        """,
                        unsafe_allow_html=True
                    )

            # M√©todo 2: ANOVA F para num√©ricas
            if "ANOVA F (Num√©ricas)" in metodos:
                st.subheader("2Ô∏è‚É£ ANOVA F para Variables Num√©ricas")

                F_scores, _ = ml_libs['f_classif'](X_num_scaled, y_encoded)
                num_importance = pd.Series(F_scores, index=X_num_scaled.columns).sort_values(ascending=False)

                cum_num = num_importance.cumsum() / num_importance.sum()
                num_vars_sel = cum_num[cum_num <= thresh].index.tolist()
                if len(num_vars_sel) < len(num_importance):
                    num_vars_sel = list(num_vars_sel) + [num_importance.index[len(num_vars_sel)]]

                results['anova'] = {
                    'scores': num_importance,
                    'selected_vars': num_vars_sel,
                    'method': 'ANOVA F'
                }

                # Visualizaci√≥n
                col1, col2 = st.columns(2)

                with col1:
                    fig = px.bar(
                        x=num_importance.head(10).index,
                        y=num_importance.head(10).values,
                        title="Top 10 Variables Num√©ricas (ANOVA F)"
                    )
                    fig.update_xaxis(tickangle=45)
                    st.plotly_chart(fig, use_container_width=True)

                with col2:
                    st.markdown(
                        f"""
                        <div class="success-card">
                            <h4>üìä Resultados ANOVA F</h4>
                            <p><strong>Variables disponibles:</strong> {len(num_importance)}</p>
                            <p><strong>Variables seleccionadas:</strong> {len(num_vars_sel)}</p>
                            <p><strong>Umbral:</strong> {thresh:.1%}</p>
                        </div>
                        """,
                        unsafe_allow_html=True
                    )

            # M√©todo 3: Random Forest (Embedded)
            if "Random Forest (Embedded)" in metodos:
                st.subheader("3Ô∏è‚É£ Random Forest (M√©todo Embedded)")

                # Combinar datos
                X_complete = pd.concat([X_num_scaled, X_cat_dummies.fillna(0)], axis=1)

                rf = ml_libs['RandomForestClassifier'](n_estimators=100, random_state=RANDOM_STATE)
                rf.fit(X_complete, y_encoded)

                importances_rf = rf.feature_importances_
                indices_rf = np.argsort(importances_rf)[::-1]
                cumulative_importance_rf = np.cumsum(importances_rf[indices_rf])

                cutoff_rf = np.where(cumulative_importance_rf >= thresh)[0][0] + 1
                selected_features_rf = X_complete.columns[indices_rf][:cutoff_rf]

                results['random_forest'] = {
                    'scores': pd.Series(importances_rf, index=X_complete.columns).sort_values(ascending=False),
                    'selected_vars': selected_features_rf.tolist(),
                    'method': 'Random Forest'
                }

                # Visualizaci√≥n
                col1, col2 = st.columns(2)

                with col1:
                    top_features = pd.Series(importances_rf, index=X_complete.columns).nlargest(10)
                    fig = px.bar(
                        x=top_features.index,
                        y=top_features.values,
                        title="Top 10 Caracter√≠sticas (Random Forest)"
                    )
                    fig.update_xaxis(tickangle=45)
                    st.plotly_chart(fig, use_container_width=True)

                with col2:
                    st.markdown(
                        f"""
                        <div class="success-card">
                            <h4>üìä Resultados Random Forest</h4>
                            <p><strong>Variables disponibles:</strong> {X_complete.shape[1]}</p>
                            <p><strong>Variables seleccionadas:</strong> {cutoff_rf}</p>
                            <p><strong>Umbral:</strong> {thresh:.1%}</p>
                        </div>
                        """,
                        unsafe_allow_html=True
                    )

            # Guardar resultados en session_state
            st.session_state['feature_selection_results'] = results

        st.success("‚úÖ Selecci√≥n de caracter√≠sticas completada!")

        # Resumen comparativo
        if len(results) > 1:
            st.subheader("üìä Comparaci√≥n de M√©todos")

            comparison_data = []
            for method, result in results.items():
                comparison_data.append({
                    'M√©todo': result['method'],
                    'Variables Seleccionadas': len(result['selected_vars']),
                    'Top Variable': result['scores'].index[0] if len(result['scores']) > 0 else 'N/A',
                    'Score Top Variable': f"{result['scores'].iloc[0]:.4f}" if len(result['scores']) > 0 else 'N/A'
                })

            comparison_df = pd.DataFrame(comparison_data)
            st.dataframe(comparison_df, use_container_width=True)

def seccion_modelado_individual():
    st.header("ü§ñ Modelado Individual")

    if 'feature_selection_results' not in st.session_state:
        st.warning("‚ö†Ô∏è Primero debes ejecutar la selecci√≥n de caracter√≠sticas.")
        return

    # Preparar datos
    X_num_scaled = st.session_state['X_num_scaled']
    X_cat_dummies = st.session_state['X_cat_dummies']
    y_encoded = st.session_state['y_encoded']
    label_encoder = st.session_state['label_encoder']

    st.subheader("Configuraci√≥n del Modelo")

    col1, col2 = st.columns(2)

    with col1:
        # Selecci√≥n de modelo
        modelo_seleccionado = st.selectbox(
            "Selecciona un modelo:",
            ["Random Forest", "Extra Trees", "Gradient Boosting", "Regresi√≥n Log√≠stica", "SVM Lineal"]
        )

        # Selecci√≥n de m√©todo de caracter√≠sticas
        metodo_caracteristicas = st.selectbox(
            "M√©todo de selecci√≥n de caracter√≠sticas:",
            list(st.session_state['feature_selection_results'].keys())
        )

        # Par√°metros de validaci√≥n
        test_size = st.slider("Tama√±o del conjunto de prueba (%)", 10, 40, 25) / 100
        cv_folds = st.slider("N√∫mero de folds para CV", 3, 10, 5)

    with col2:
        st.markdown("###