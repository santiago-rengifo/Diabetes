# -*- coding: utf-8 -*-
"""APP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1az9rbFwqphO6aDAhL0OhFleAJdR5ciJS
"""

# app.py
# Streamlit wrapper of the Diabetes pipeline with interactive controls
# Adapted from the user's diabetes_v8 workflow. See sidebar for options.

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import streamlit as st

import matplotlib.pyplot as plt
import seaborn as sns

from ucimlrepo import fetch_ucirepo

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

from sklearn.decomposition import PCA
import mca

from sklearn.feature_selection import chi2, f_classif
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import (
    classification_report, confusion_matrix, ConfusionMatrixDisplay,
    roc_curve, auc
)

from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import ADASYN, SMOTE

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.utils.class_weight import compute_class_weight
from scipy.stats import randint, uniform


# -------------------------
# Streamlit page config
# -------------------------
st.set_page_config(
    page_title="Diabetes ML ‚Äì Interactive",
    page_icon="ü©∫",
    layout="wide"
)

st.title("ü©∫ Diabetes ML ‚Äì Interactive Pipeline")
st.caption("Feature selection ‚Üí PCA/MCA ‚Üí Rebalancing ‚Üí Model training ‚Üí Metrics & plots")

RANDOM_STATE_DEFAULT = 42


# -------------------------
# Sidebar controls
# -------------------------
st.sidebar.header("‚öôÔ∏è Controls")

seed = st.sidebar.number_input("Random Seed", value=RANDOM_STATE_DEFAULT, step=1)
sample_n = st.sidebar.slider("Sample rows (for speed)", min_value=3000, max_value=100000, value=20000, step=1000)

thresh = st.sidebar.slider("Cumulative importance / variance threshold", min_value=0.70, max_value=0.99, value=0.90, step=0.01)

feat_strategy = st.sidebar.selectbox(
    "Feature selection strategy",
    ["Filter (ANOVA for numeric + Chi¬≤ for categoricals)", "Embedded (Random Forest)"]
    # RFECV omitted by default for speed; can be added as a third option if needed
)

pca_enable = st.sidebar.checkbox("Apply PCA to numeric block", value=True)
mca_enable = st.sidebar.checkbox("Apply MCA to categorical block", value=True)

balancing = st.sidebar.selectbox(
    "Rebalancing method",
    ["None", "class_weight=balanced (if supported)", "SMOTE(k=3)", "ADASYN"]
)

model_name = st.sidebar.selectbox(
    "Model",
    ["RandomForest", "ExtraTrees", "HistGradientBoosting", "LogisticRegression", "SVM_Linear"]
)

param_mode = st.sidebar.radio("Hyperparameter search", ["Fast (quick grids)", "Full (wider grids)"], index=0)
n_iter = st.sidebar.slider("RandomizedSearch n_iter", min_value=5, max_value=50, value=10, step=5)
cv_folds = st.sidebar.slider("CV folds", min_value=3, max_value=10, value=3, step=1)

test_size = st.sidebar.slider("Test size", min_value=0.1, max_value=0.4, value=0.25, step=0.05)

st.sidebar.markdown("---")
st.sidebar.markdown("**Tips**")
st.sidebar.caption("- Reduce sample size for speed\n- Use *Fast* grids first\n- MCA can be slow on many categories")


# -------------------------
# Cached data loader
# -------------------------
@st.cache_data(show_spinner=False)
def load_data(seed: int, sample_n: int):
    ds = fetch_ucirepo(id=296)  # Diabetes 130-US hospitals
    X_raw: pd.DataFrame = ds.data.features.copy()
    y_raw: pd.DataFrame = ds.data.targets.copy()

    # Drop id-like / diagnostic-heavy columns
    id_like = [c for c in X_raw.columns if c.lower() in
               ("encounter_id", "patient_nbr", "encounterid", "patientnbr", "diag_1", "diag_2", "diag_3",
                "payer_code", "medical_specialty")]
    X_raw = X_raw.drop(columns=id_like, errors="ignore")

    # Uniform dtypes for splits
    X_num = X_raw.select_dtypes(include=[np.number]).copy()
    X_cat = X_raw.select_dtypes(include=['object', 'category']).copy()

    # Create a stable sample
    if sample_n < len(X_raw):
        np.random.seed(seed)
        idx = np.random.choice(X_raw.index, size=sample_n, replace=False)
        X_raw = X_raw.loc[idx].reset_index(drop=True)
        y_raw = y_raw.loc[idx].reset_index(drop=True)
        X_num = X_raw.select_dtypes(include=[np.number]).copy()
        X_cat = X_raw.select_dtypes(include=['object', 'category']).copy()
    else:
        X_raw = X_raw.reset_index(drop=True)
        y_raw = y_raw.reset_index(drop=True)

    return X_raw, y_raw, X_num, X_cat


# -------------------------
# Feature selection helpers
# -------------------------
def prepare_blocks(X_num: pd.DataFrame, X_cat: pd.DataFrame):
    # Numeric impute
    Xnum_imp = X_num.copy()
    for c in Xnum_imp.columns:
        if not np.issubdtype(Xnum_imp[c].dtype, np.number):
            Xnum_imp[c] = pd.to_numeric(Xnum_imp[c], errors='coerce')
    Xnum_imp = Xnum_imp.fillna(Xnum_imp.median(numeric_only=True))

    # Categorical to strings
    Xcat_imp = X_cat.fillna('Missing').astype(str)
    return Xnum_imp, Xcat_imp


def filter_selection(Xnum_imp, Xcat_imp, y_enc, thresh):
    # Chi2 per categorical variable (aggregate dummy chi2 within original var)
    Xcat_dum_all = pd.get_dummies(Xcat_imp, drop_first=False, prefix_sep='=')
    chi2_scores, _ = chi2(Xcat_dum_all.fillna(0), y_enc)
    var_of_dummy = Xcat_dum_all.columns.to_series().str.split('=', n=1, expand=True)[0]
    var_importance_cat = (
        pd.DataFrame({'var': var_of_dummy, 'score': chi2_scores})
        .groupby('var', sort=False)['score']
        .sum()
        .sort_values(ascending=False)
    )
    cum_cat = var_importance_cat.cumsum() / var_importance_cat.sum()
    cat_vars_sel = cum_cat[cum_cat <= thresh].index.tolist()
    if len(cat_vars_sel) < len(var_importance_cat):
        cat_vars_sel = cat_vars_sel + [var_importance_cat.index[len(cat_vars_sel)]]
    X_cat_sel = Xcat_imp[cat_vars_sel]
    X_cat_sel_dum = pd.get_dummies(X_cat_sel, drop_first=True, prefix_sep='=')

    # ANOVA F for numerics
    F_scores, _ = f_classif(Xnum_imp, y_enc)
    num_importance = pd.Series(F_scores, index=Xnum_imp.columns).sort_values(ascending=False)
    cum_num = num_importance.cumsum() / num_importance.sum()
    num_vars_sel = cum_num[cum_num <= thresh].index.tolist()
    if len(num_vars_sel) < len(num_importance):
        num_vars_sel = list(num_vars_sel) + [num_importance.index[len(num_vars_sel)]]
    X_num_sel = Xnum_imp[num_vars_sel]

    df_filtrado = pd.concat([X_num_sel.reset_index(drop=True),
                             X_cat_sel_dum.reset_index(drop=True)], axis=1)
    details = {
        "cat_vars_sel": cat_vars_sel,
        "num_vars_sel": list(num_vars_sel),
        "dummies_cat": X_cat_sel_dum.shape[1],
        "var_importance_cat": var_importance_cat,
        "num_importance": num_importance
    }
    return df_filtrado, details, X_cat_sel  # return X_cat_sel (strings) for MCA


def embedded_rf_selection(Xnum_imp, Xcat_imp, y_enc, thresh, seed):
    Xcat_dum_all = pd.get_dummies(Xcat_imp, drop_first=False, prefix_sep='=').fillna(0)
    X_complete = pd.concat([Xnum_imp, Xcat_dum_all], axis=1)

    rf = RandomForestClassifier(n_estimators=200, random_state=seed, n_jobs=-1)
    rf.fit(X_complete, y_enc)
    importances = rf.feature_importances_
    idx = np.argsort(importances)[::-1]
    cumsum = np.cumsum(importances[idx])
    cutoff = np.where(cumsum >= thresh)[0][0] + 1
    selected_cols = X_complete.columns[idx][:cutoff]
    df_filtrado = X_complete[selected_cols].copy()

    # For MCA we still need the original categorical subset
    cat_cols = [c for c in selected_cols if c in Xcat_dum_all.columns]
    # Map back to original categorical variable names
    base_cats = sorted(set([c.split('=')[0] for c in cat_cols]))
    X_cat_sel = Xcat_imp[base_cats] if base_cats else pd.DataFrame(index=Xcat_imp.index)

    details = {
        "selected_features": list(selected_cols),
        "cutoff_features": cutoff,
        "rf_importances_sorted": pd.Series(importances[idx], index=X_complete.columns[idx])
    }
    return df_filtrado, details, X_cat_sel


def pca_mca_blocks(df_filtrado, X_cat_sel, apply_pca, apply_mca, thresh, seed):
    parts = []
    info = {}

    # Numeric PCA
    if apply_pca:
        df_num = df_filtrado.select_dtypes(include=[np.number])
        if df_num.shape[1] > 0:
            scaler = StandardScaler()
            Xn = scaler.fit_transform(df_num)
            p = PCA(random_state=seed).fit(Xn)
            cum = np.cumsum(p.explained_variance_ratio_)
            r = int(np.searchsorted(cum, thresh) + 1)
            Xp = p.transform(Xn)[:, :r]
            pca_cols = [f"PC{i+1}" for i in range(r)]
            parts.append(pd.DataFrame(Xp, columns=pca_cols, index=df_filtrado.index))
            info["pca_kept"] = r
            info["pca_var"] = float(cum[r-1])
        else:
            info["pca_kept"] = 0
            info["pca_var"] = 0.0

    # Categorical MCA (use selected original categorical columns)
    if apply_mca and X_cat_sel is not None and X_cat_sel.shape[1] > 0:
        Xcat_imp = X_cat_sel.fillna('Missing').astype(str)
        Xcat_dum = pd.get_dummies(Xcat_imp, drop_first=False, prefix_sep='=')
        try:
            m = mca.MCA(Xcat_dum, benzecri=True)
            sv = m.s
            eig = sv ** 2
            expl = eig / eig.sum()
            cum = np.cumsum(expl)
            r = int(np.searchsorted(cum, thresh) + 1)
            # Coordinates of rows
            try:
                F = m.fs_r(N=r)
            except Exception:
                F = m.fs_r()[:, :r]
            mca_cols = [f"MCA{i+1}" for i in range(r)]
            parts.append(pd.DataFrame(F, columns=mca_cols, index=X_cat_sel.index))
            info["mca_kept"] = r
            info["mca_var"] = float(cum[r-1])
        except Exception as e:
            st.warning(f"MCA failed or too sparse; skipping MCA. Reason: {e}")
            info["mca_kept"] = 0
            info["mca_var"] = 0.0
    else:
        info["mca_kept"] = 0
        info["mca_var"] = 0.0

    if parts:
        df_combined = pd.concat(parts, axis=1)
    else:
        # If neither PCA nor MCA, just pass df_filtrado
        df_combined = df_filtrado.copy()

    return df_combined, info


# -------------------------
# Models & search spaces
# -------------------------
def get_models(seed):
    models = {
        "RandomForest": RandomForestClassifier(random_state=seed),
        "ExtraTrees": ExtraTreesClassifier(random_state=seed),
        "HistGradientBoosting": HistGradientBoostingClassifier(random_state=seed),
        "LogisticRegression": LogisticRegression(random_state=seed, max_iter=1000, solver='liblinear'),
        "SVM_Linear": SVC(kernel='linear', random_state=seed, probability=True, max_iter=1000),
    }
    return models


def get_param_spaces(mode):
    full = {
        "RandomForest": {
            'classifier__n_estimators': randint(100, 400),
            'classifier__max_depth': [10, 20, 30, None],
            'classifier__min_samples_split': randint(2, 10),
            'classifier__min_samples_leaf': randint(1, 5),
            'classifier__max_features': ['sqrt', 'log2']
        },
        "ExtraTrees": {
            'classifier__n_estimators': randint(100, 400),
            'classifier__max_depth': [10, 20, 30, None],
            'classifier__min_samples_split': randint(2, 10),
            'classifier__min_samples_leaf': randint(1, 5),
            'classifier__max_features': ['sqrt', 'log2']
        },
        "HistGradientBoosting": {
            'classifier__max_iter': randint(50, 250),
            'classifier__learning_rate': uniform(0.05, 0.25),
            'classifier__max_depth': randint(3, 10),
            'classifier__max_leaf_nodes': [31, 50, 100, 200]
        },
        "LogisticRegression": {
            'classifier__C': uniform(0.1, 10),
            'classifier__penalty': ['l1', 'l2'],
            'classifier__solver': ['liblinear', 'saga'],
            'classifier__max_iter': [1000, 2000]
        },
        "SVM_Linear": {
            'classifier__C': uniform(0.1, 10),
            'classifier__max_iter': [1000, 2000]
        }
    }
    fast = {
        "RandomForest": {
            'classifier__n_estimators': [100, 200],
            'classifier__max_depth': [10, 20],
        },
        "ExtraTrees": {
            'classifier__n_estimators': [100, 200],
            'classifier__max_depth': [10, 20],
        },
        "HistGradientBoosting": {
            'classifier__max_iter': [60, 120],
            'classifier__learning_rate': [0.1, 0.2],
        },
        "LogisticRegression": {
            'classifier__C': [0.1, 1, 10],
            'classifier__penalty': ['l2'],
            'classifier__solver': ['liblinear']
        },
        "SVM_Linear": {
            'classifier__C': [0.1, 1, 10]
        }
    }
    return fast if mode.startswith("Fast") else full


def build_preprocessor(columns):
    # only numeric columns incoming (PCA/MCA outputs + dummies/PCA as numeric)
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, columns)])
    return preprocessor


def apply_balancing_choice(balancing, model, y_train):
    steps = []
    info = ""

    if balancing == "SMOTE(k=3)":
        steps.append(('balancer', SMOTE(random_state=RANDOM_STATE_DEFAULT, k_neighbors=3)))
        info = "SMOTE(k=3)"
    elif balancing == "ADASYN":
        steps.append(('balancer', ADASYN(random_state=RANDOM_STATE_DEFAULT)))
        info = "ADASYN"
    elif balancing == "class_weight=balanced (if supported)":
        if hasattr(model, "class_weight"):
            weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
            model.set_params(class_weight=dict(zip(np.unique(y_train), weights)))
            info = "class_weight=balanced"
        else:
            info = "class_weight requested but not supported; ignored"
    else:
        info = "None"
    return steps, info


# -------------------------
# MAIN APP
# -------------------------
with st.spinner("Loading dataset..."):
    X_raw, y_raw, X_num, X_cat = load_data(seed, sample_n)

# Target encoding
y_series = y_raw.iloc[:, 0].astype(str)
le_original = LabelEncoder().fit(y_series)
y_enc = le_original.transform(y_series)
class_names = list(le_original.classes_)

st.subheader("Dataset snapshot")
c1, c2, c3 = st.columns(3)
c1.metric("Rows", len(X_raw))
c2.metric("Numeric cols", X_num.shape[1])
c3.metric("Categorical cols", X_cat.shape[1])
st.dataframe(pd.concat([X_num.head(3), X_cat.head(3)], axis=1))

with st.expander("Class distribution (full sample)"):
    st.write(pd.Series(y_series).value_counts())

# Prepare clean blocks
Xnum_imp, Xcat_imp = prepare_blocks(X_num, X_cat)

# Feature selection
with st.spinner("Selecting features..."):
    if feat_strategy.startswith("Filter"):
        df_filtrado, details, X_cat_sel_for_mca = filter_selection(Xnum_imp, Xcat_imp, y_enc, thresh)
    else:
        df_filtrado, details, X_cat_sel_for_mca = embedded_rf_selection(Xnum_imp, Xcat_imp, y_enc, thresh, seed)

st.subheader("Selected features")
st.write(f"Resulting matrix: {df_filtrado.shape[0]} rows √ó {df_filtrado.shape[1]} columns")
if feat_strategy.startswith("Filter"):
    st.caption(f"üßÆ Numeric kept: {len(details['num_vars_sel'])} | Categorical kept: {len(details['cat_vars_sel'])} ‚Üí dummies: {details['dummies_cat']}")
else:
    st.caption(f"üß† Embedded RF kept top {len(details['selected_features'])} features (‚â• {int(thresh*100)}% cumulative importance)")

with st.expander("Top importances / stats"):
    if feat_strategy.startswith("Filter"):
        st.write("Top categorical (Chi¬≤ aggregated):")
        st.write(details['var_importance_cat'].head(15))
        st.write("Top numeric (ANOVA F):")
        st.write(details['num_importance'].head(15))
    else:
        st.write("Top RF importances:")
        st.write(details['rf_importances_sorted'].head(25))

# PCA + MCA
with st.spinner("Computing PCA/MCA blocks..."):
    df_combined, decomp_info = pca_mca_blocks(df_filtrado, X_cat_sel_for_mca, pca_enable, mca_enable, thresh, seed)

st.subheader("Dimensionality reduction")
st.write(f"Combined matrix used for training: {df_combined.shape}")
colA, colB = st.columns(2)
colA.caption(f"PCA kept: {decomp_info.get('pca_kept', 0)} comps (var‚âà{decomp_info.get('pca_var', 0):.3f})")
colB.caption(f"MCA kept: {decomp_info.get('mca_kept', 0)} dims (var‚âà{decomp_info.get('mca_var', 0):.3f})")

if df_combined.shape[1] >= 2:
    with st.expander("PC1 vs PC2 / MCA1 vs MCA2 scatter (if available)"):
        fig, ax = plt.subplots(figsize=(5.5, 5))
        # best-effort: use first two columns of combined
        scatter = ax.scatter(df_combined.iloc[:, 0], df_combined.iloc[:, 1],
                             c=y_enc, alpha=0.6)
        ax.set_xlabel(df_combined.columns[0]); ax.set_ylabel(df_combined.columns[1])
        ax.set_title("First two components (combined)")
        st.pyplot(fig)

# Train / test split
X_train, X_test, y_train, y_test = train_test_split(
    df_combined, y_enc, test_size=test_size, stratify=y_enc, random_state=seed
)

st.subheader("Train/Test split")
c1, c2 = st.columns(2)
c1.write(f"X_train: {X_train.shape}, y_train: {pd.Series(y_train).value_counts().to_dict()}")
c2.write(f"X_test: {X_test.shape}, y_test: {pd.Series(y_test).value_counts().to_dict()}")

# Build pipeline
models = get_models(seed)
clf = models[model_name]
preprocessor = build_preprocessor(df_combined.columns.tolist())

balance_steps, balance_info = apply_balancing_choice(balancing, clf, y_train)
Pipe = ImbPipeline if any(s[0] == 'balancer' for s in balance_steps) else Pipeline
pipeline = Pipe([('preprocessor', preprocessor), *balance_steps, ('classifier', clf)])

param_spaces = get_param_spaces(param_mode)
param_grid = param_spaces.get(model_name, {})

# Train
st.subheader("Training")
with st.spinner(f"Training {model_name} ({param_mode})..."):
    if param_grid:
        search = RandomizedSearchCV(
            pipeline,
            param_distributions=param_grid,
            n_iter=n_iter,
            cv=cv_folds,
            scoring='f1_macro',
            random_state=seed,
            n_jobs=-1,
            error_score='raise'
        )
        search.fit(X_train, y_train)
        best_model = search.best_estimator_
        best_cv = search.best_score_
        st.success(f"Best CV f1_macro: {best_cv:.4f}")
        st.caption(f"Best params: {search.best_params_}")
    else:
        st.info("No param grid found for this model. Fitting defaults.")
        pipeline.fit(X_train, y_train)
        best_model = pipeline
        best_cv = None

# Evaluate
st.subheader("Evaluation on Test Set")
y_pred = best_model.predict(X_test)
proba_ok = hasattr(best_model, "predict_proba")
y_proba = best_model.predict_proba(X_test) if proba_ok else None

report = classification_report(y_test, y_pred, target_names=class_names, output_dict=False, zero_division=0)
st.text(report)

cm = confusion_matrix(y_test, y_pred)
fig_cm, ax_cm = plt.subplots(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap="Blues", values_format='d', ax=ax_cm, colorbar=False)
ax_cm.set_title(f"Confusion Matrix ‚Äì {model_name}")
st.pyplot(fig_cm)

# ROC (only if binary)
if y_proba is not None and len(np.unique(y_test)) == 2:
    fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])
    roc_auc = auc(fpr, tpr)
    fig_roc, ax_roc = plt.subplots(figsize=(6, 5))
    ax_roc.plot(fpr, tpr, lw=2, label=f"AUC={roc_auc:.3f}")
    ax_roc.plot([0, 1], [0, 1], linestyle="--")
    ax_roc.set_xlabel("FPR"); ax_roc.set_ylabel("TPR"); ax_roc.legend()
    ax_roc.set_title("ROC Curve (binary only)")
    st.pyplot(fig_roc)
else:
    st.caption("ROC not shown (multi-class).")

# Download predictions
out = pd.DataFrame({
    "y_true": [class_names[i] for i in y_test],
    "y_pred": [class_names[i] for i in y_pred]
})
if y_proba is not None:
    # add per-class probabilities
    proba_df = pd.DataFrame(y_proba, columns=[f"p_{c}" for c in class_names])
    out = pd.concat([out.reset_index(drop=True), proba_df.reset_index(drop=True)], axis=1)

st.download_button(
    "‚¨áÔ∏è Download test predictions (CSV)",
    data=out.to_csv(index=False).encode("utf-8"),
    file_name="predictions_test.csv",
    mime="text/csv"
)

# Meta
st.markdown("---")
st.caption(f"Model: **{model_name}** | Balancing: **{balance_info}** | Search: **{param_mode}** | Seed: **{seed}**")