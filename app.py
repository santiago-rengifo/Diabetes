# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1az9rbFwqphO6aDAhL0OhFleAJdR5ciJS
"""

# -*- coding: utf-8 -*-
"""
App de Streamlit/CLI – Diabetes (v9) – Pipeline de ML + EDA (versión fiel e interactiva)

Corrección de error: `ModuleNotFoundError: No module named 'streamlit'`.
- Si **Streamlit no está instalado**, la app **cae automáticamente en modo CLI** con un shim ligero que
  imprime los pasos y **guarda las figuras en `./outputs/`**, para que puedas ejecutar en entornos sandbox.
- La **carga de datos es exclusivamente desde UCI Repo (id=296)**. Si `ucimlrepo` no está disponible,
  se muestra un error y la ejecución se detiene (no hay datasets alternos para respetar tu requisito).
- EDA en español, pipelines, búsqueda de hiperparámetros y métricas; fiel a tu `diabetes_v9.py` pero interactivo.

Cómo ejecutar:
1) **Con UI (recomendado)** → `pip install streamlit ucimlrepo scikit-learn imbalanced-learn matplotlib pandas numpy seaborn` → `streamlit run app_diabetes_v9.py`
2) **Sin UI (CLI)** → `pip install ucimlrepo scikit-learn imbalanced-learn matplotlib pandas numpy seaborn` → `python app_diabetes_v9.py`

Autor: Adaptado para Santiago Rengifo (2025-08-30)
"""

import os
import sys
import time
from collections import Counter
from typing import List, Tuple, Optional, Dict, Any

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ------------------------------------------------------------
# Importación de Streamlit con fallback CLI (shim)
# ------------------------------------------------------------
NO_STREAMLIT = False
try:
    import streamlit as st  # type: ignore
except ModuleNotFoundError:
    NO_STREAMLIT = True

    class _DummyArea:
        def __init__(self, root):
            self._root = root
        def __enter__(self):
            return self
        def __exit__(self, exc_type, exc, tb):
            return False
        # Pasarelas a métodos de st
        def write(self, *args, **kwargs):
            print(*args)
        def dataframe(self, df, use_container_width: bool = True):
            if hasattr(df, 'head'):
                print(df.head(10).to_string())
            else:
                print(df)
        def code(self, x):
            print(x)
        def info(self, x):
            print(f"[info] {x}")
        def warning(self, x):
            print(f"[warning] {x}")
        def error(self, x):
            print(f"[error] {x}")

    class _DummyProgress:
        def progress(self, v: float, text: str = ""):
            print(f"[progreso] {v:.0%} {text}")

    class _DummySpinner:
        def __init__(self, text: str = ""):
            self.text = text
        def __enter__(self):
            print(f"[cargando] {self.text}")
            return self
        def __exit__(self, exc_type, exc, tb):
            return False

    class _STShim:
        def set_page_config(self, **kwargs):
            pass
        def title(self, x):
            print(f"\n=== {x} ===")
        def caption(self, x):
            print(x)
        def header(self, x):
            print(f"\n# {x}")
        def subheader(self, x):
            print(f"\n## {x}")
        def write(self, *args, **kwargs):
            print(*args)
        def dataframe(self, df, use_container_width: bool = True):
            if hasattr(df, 'head'):
                print(df.head(10).to_string())
            else:
                print(df)
        def code(self, x):
            print(x)
        def warning(self, x):
            print(f"[warning] {x}")
        def info(self, x):
            print(f"[info] {x}")
        def error(self, x):
            print(f"[error] {x}")
        def success(self, x):
            print(f"[ok] {x}")
        def pyplot(self, fig):
            try:
                os.makedirs("outputs", exist_ok=True)
                fname = os.path.join("outputs", f"fig_{int(time.time()*1000)}.png")
                fig.savefig(fname, bbox_inches="tight", dpi=120)
                print(f"[figura guardada] {fname}")
                plt.close(fig)  # Close figure to free memory
            except Exception as e:
                print(f"[error guardando figura] {e}")
        def button(self, label):
            # En CLI ejecutamos automáticamente
            print(f"[botón] {label} → ejecución automática en CLI")
            return True
        def slider(self, label, minv, maxv, value, step=None):
            return value
        def number_input(self, label, value=0, step=1):
            return value
        def multiselect(self, label, options, default=None):
            return list(options if default is None else default)
        def checkbox(self, label, value=False):
            return value
        def radio(self, label, options, help=None):
            return options[0]
        def divider(self):
            pass
        def expander(self, *args, **kwargs):
            return _DummyArea(self)
        def columns(self, spec):
            n = len(spec) if hasattr(spec, "__len__") else int(spec)
            return [_DummyArea(self) for _ in range(n)]
        def progress(self, *args, **kwargs):
            return _DummyProgress()
        def spinner(self, text: str = ""):
            return _DummySpinner(text)
        @property
        def sidebar(self):
            return _DummyArea(self)
        def stop(self):
            raise SystemExit()
        # Cache-data de Streamlit → no-op
        def cache_data(self, show_spinner: bool = False):
            def deco(fn):
                return fn
            return deco

    st = _STShim()  # type: ignore

# ------------------------------------------------------------
# Dependencias opcionales (seaborn)
# ------------------------------------------------------------
HAS_SEABORN = True
try:
    import seaborn as sns  # type: ignore
except Exception:
    HAS_SEABORN = False

# ------------------------------------------------------------
# UCI Repo (solo carga por este medio)
# ------------------------------------------------------------
HAS_UCI = True
try:
    from ucimlrepo import fetch_ucirepo  # type: ignore
except Exception:
    HAS_UCI = False

# Sklearn imports with error handling
try:
    from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
    from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, FunctionTransformer
    from sklearn.impute import SimpleImputer
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.decomposition import PCA, TruncatedSVD
    from sklearn.feature_selection import SelectFromModel

    from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC

    from sklearn.metrics import (
        classification_report,
        confusion_matrix,
        ConfusionMatrixDisplay,
        f1_score,
        precision_recall_curve,
        average_precision_score,
        roc_curve,
        auc,
    )
    from sklearn.preprocessing import label_binarize
except ImportError as e:
    st.error(f"Error importing sklearn modules: {e}")
    st.stop()

try:
    from imblearn.pipeline import Pipeline as ImbPipeline
    from imblearn.over_sampling import SMOTENC, SMOTE
except ImportError as e:
    st.error(f"Error importing imbalanced-learn modules: {e}")
    st.stop()

RANDOM_STATE_DEFAULT = 42

# ==========================
# CONFIGURACIÓN DE PÁGINA/UI
# ==========================
if not NO_STREAMLIT:
    try:
        st.set_page_config(
            page_title="Diabetes v9 – Pipeline ML + EDA",
            page_icon="🩺",
            layout="wide",
            initial_sidebar_state="expanded",
        )
    except Exception as e:
        st.warning(f"Could not set page config: {e}")

st.title("🩺 Diabetes – Pipeline de ML (v9) + EDA")
st.caption("Versión fiel e interactiva del código, con EDA y carga **exclusiva** desde UCI Repo (id=296).")

# ==========================
# FUNCIONES AUXILIARES (compatibilidad y EDA)
# ==========================

def make_ohe():
    """Devuelve OneHotEncoder compatible con scikit-learn nuevo/antiguo."""
    try:
        return OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    except TypeError:
        try:
            return OneHotEncoder(handle_unknown="ignore", sparse=False)
        except TypeError:
            return OneHotEncoder(handle_unknown="ignore")

@st.cache_data(show_spinner=False)
def cargar_uci() -> Tuple[pd.DataFrame, pd.Series]:
    """Carga el dataset desde UCI Repo (id=296). **Sin alternativas**."""
    if not HAS_UCI:
        st.error("`ucimlrepo` no está disponible. Instálalo para cargar desde UCI Repo.")
        st.stop()

    try:
        ds = fetch_ucirepo(id=296)
        X = ds.data.features.copy()
        y = ds.data.targets.copy()

        # Handle target variable
        if hasattr(y, "shape") and len(getattr(y, "shape", [])) == 2 and y.shape[1] == 1:
            y = y.iloc[:, 0]
        elif hasattr(y, "shape") and len(getattr(y, "shape", [])) == 2:
            # If multiple targets, take the first one
            y = y.iloc[:, 0]

        y = y.astype(str)
        return X, y
    except Exception as e:
        st.error(f"Error loading UCI dataset: {e}")
        st.stop()

@st.cache_data(show_spinner=False)
def inferir_tipos(df: pd.DataFrame) -> Tuple[List[str], List[str]]:
    """Infer column types safely"""
    cat_cols = []
    num_cols = []

    for col in df.columns:
        try:
            if pd.api.types.is_object_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):
                cat_cols.append(col)
            else:
                num_cols.append(col)
        except Exception:
            # If we can't determine type, assume it's categorical to be safe
            cat_cols.append(col)

    return num_cols, cat_cols

def safe_plot_close():
    """Safely close matplotlib plots"""
    try:
        plt.close('all')
    except Exception:
        pass

# ==========================
# BARRA LATERAL – PARÁMETROS
# ==========================
with st.sidebar:
    st.header("1) Datos")
    st.write("Fuente: **UCI Repo (id=296)** – sin opción de CSV.")

    st.divider()
    st.header("2) Partición y validación")
    test_size = st.slider("Tamaño de test", 0.1, 0.4, 0.25, 0.05)
    random_state = st.number_input("Semilla aleatoria", value=RANDOM_STATE_DEFAULT, step=1)
    n_splits = st.slider("Folds (StratifiedKFold)", 2, 10, 5, 1)

    st.divider()
    st.header("3) Modelos a evaluar")
    modelos_disponibles = [
        "RandomForest",
        "ExtraTrees",
        "HistGradientBoosting",
        "LogisticRegression",
        "SVM_Linear",
    ]
    modelos_seleccionados = st.multiselect("Modelos", modelos_disponibles, default=["RandomForest", "LogisticRegression"])

    st.divider()
    st.header("4) Opciones avanzadas")
    n_iter = st.slider("Iteraciones de RandomizedSearchCV", 5, 30, 10, 5)  # Reduced default
    usar_selector = st.checkbox("Usar SelectFromModel(ExtraTrees)", value=True)
    permitir_smote_lineales = st.checkbox("Permitir SMOTE/SMOTENC en modelos lineales (LR/SVM)", value=True)
    graficar_pr = st.checkbox("Graficar curvas Precision–Recall (OvR)", value=False)

    st.divider()
    st.header("5) Controles de EDA")
    max_cols_hist = st.slider("Máx. variables numéricas para histogramas", 1, 20, 6, 1)  # Reduced default
    corr_top_k = st.slider("Top-k variables para matriz de correlación (por varianza)", 3, 20, 8, 1)  # Reduced default

# ==========================
# CARGA DE DATOS (UCI **obligatorio**)
# ==========================
try:
    with st.spinner("Cargando datos desde UCI Repo..."):
        X_full, y_full = cargar_uci()
except SystemExit:
    sys.exit(1)
except Exception as e:
    st.error(f"No se pudo cargar desde **UCI Repo**: {e}")
    st.error("Asegúrate de instalar `ucimlrepo` y tener conexión a internet.")
    st.stop()

# Validate data
if X_full.empty or y_full.empty:
    st.error("Los datos cargados están vacíos.")
    st.stop()

num_cols, cat_cols = inferir_tipos(X_full)

with st.expander("🔍 EDA – Vista general", expanded=True):
    c1, c2, c3, c4 = st.columns([1.2,1,1,1])
    with c1:
        st.write("**Dimensiones**: X = ", X_full.shape, " | y = ", y_full.shape)
        st.write("**Clases**:")
        try:
            class_counts = Counter(y_full)
            st.write(class_counts)
        except Exception as e:
            st.write(f"Error counting classes: {e}")
    with c2:
        st.write("**Nº variables numéricas**:", len(num_cols))
        st.write("**Nº variables categóricas**:", len(cat_cols))
    with c3:
        try:
            pct_null = X_full.isna().mean().mean()*100
            st.write(f"**% faltantes promedio en X**: {pct_null:.2f}%")
        except Exception as e:
            st.write(f"Error calculating missing values: {e}")
    with c4:
        st.write("**Ejemplo (primeras filas)**:")
        try:
            st.dataframe(X_full.head(5))
        except Exception as e:
            st.write(f"Error displaying data: {e}")

with st.expander("📉 EDA – Faltantes por columna", expanded=False):
    try:
        na_pct = X_full.isna().mean().sort_values(ascending=False)
        df_na = na_pct.to_frame("porcentaje_faltantes").reset_index()
        df_na.columns = ["columna", "porcentaje_faltantes"]
        st.dataframe(df_na, use_container_width=True)

        if len(df_na) > 0:
            fig_na, ax_na = plt.subplots(figsize=(8,min(6, len(df_na)*0.3 + 2)))
            ax_na.barh(df_na["columna"], df_na["porcentaje_faltantes"])
            ax_na.set_xlabel("Porcentaje de faltantes")
            ax_na.set_title("Faltantes por columna")
            plt.tight_layout()
            st.pyplot(fig_na)
            safe_plot_close()
    except Exception as e:
        st.error(f"Error en análisis de faltantes: {e}")

with st.expander("📊 EDA – Histogramas (variables numéricas)", expanded=False):
    if len(num_cols) == 0:
        st.info("No hay variables numéricas.")
    else:
        try:
            # Seleccionar por varianza para no sobrecargar
            numeric_data = X_full[num_cols].select_dtypes(include=[np.number])
            if not numeric_data.empty:
                var_order = numeric_data.var().sort_values(ascending=False).index.tolist()
                cols_plot = var_order[:min(max_cols_hist, len(var_order))]

                ncols = min(2, len(cols_plot))
                nrows = int(np.ceil(len(cols_plot)/ncols))

                if nrows > 0 and ncols > 0:
                    fig_h, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 3*nrows))
                    if nrows == 1 and ncols == 1:
                        axes = [axes]
                    elif nrows == 1:
                        axes = axes
                    else:
                        axes = axes.flatten()

                    for i, col in enumerate(cols_plot):
                        ax = axes[i]
                        data = X_full[col].dropna()
                        if len(data) > 0:
                            ax.hist(data, bins=min(30, len(data)//10 + 1))
                            ax.set_title(col)

                    # Hide unused subplots
                    for j in range(len(cols_plot), len(axes)):
                        axes[j].axis('off')

                    plt.tight_layout()
                    st.pyplot(fig_h)
                    safe_plot_close()
            else:
                st.info("No hay variables numéricas válidas para histogramas.")
        except Exception as e:
            st.error(f"Error en histogramas: {e}")

with st.expander("📈 EDA – Boxplots (variables numéricas)", expanded=False):
    if len(num_cols) == 0:
        st.info("No hay variables numéricas.")
    else:
        try:
            numeric_data = X_full[num_cols].select_dtypes(include=[np.number])
            if not numeric_data.empty:
                cols_plot = numeric_data.columns[:min(8, len(numeric_data.columns))]
                fig_b, ax_b = plt.subplots(figsize=(10,4))

                if HAS_SEABORN:
                    import seaborn as sns  # type: ignore
                    sns.boxplot(data=X_full[cols_plot], ax=ax_b)
                else:
                    plot_data = []
                    labels = []
                    for col in cols_plot:
                        data = X_full[col].dropna()
                        if len(data) > 0:
                            plot_data.append(data)
                            labels.append(col)

                    if plot_data:
                        ax_b.boxplot(plot_data, labels=labels)

                ax_b.set_title("Boxplots (muestra de variables)")
                plt.xticks(rotation=45)
                plt.tight_layout()
                st.pyplot(fig_b)
                safe_plot_close()
            else:
                st.info("No hay variables numéricas válidas para boxplots.")
        except Exception as e:
            st.error(f"Error en boxplots: {e}")

with st.expander("🔗 EDA – Matriz de correlación (Top-k por varianza)", expanded=False):
    if len(num_cols) < 2:
        st.info("Se requieren al menos 2 variables numéricas para correlación.")
    else:
        try:
            numeric_data = X_full[num_cols].select_dtypes(include=[np.number])
            if len(numeric_data.columns) >= 2:
                var_order = numeric_data.var().sort_values(ascending=False).index.tolist()
                sel = var_order[:min(corr_top_k, len(var_order))]
                corr = X_full[sel].corr(numeric_only=True)

                if not corr.empty:
                    fig_c, ax_c = plt.subplots(figsize=(min(12, len(sel)+2), min(12, len(sel)+2)))

                    if HAS_SEABORN:
                        import seaborn as sns  # type: ignore
                        sns.heatmap(corr, annot=len(sel) <= 10, cmap="RdBu_r", center=0, ax=ax_c, fmt='.2f')
                    else:
                        im = ax_c.imshow(corr.values, vmin=-1, vmax=1, cmap='RdBu_r')
                        ax_c.set_xticks(range(len(sel)))
                        ax_c.set_yticks(range(len(sel)))
                        ax_c.set_xticklabels(sel, rotation=90)
                        ax_c.set_yticklabels(sel)
                        fig_c.colorbar(im)

                    ax_c.set_title("Correlación (Top-k por varianza)")
                    plt.tight_layout()
                    st.pyplot(fig_c)
                    safe_plot_close()
                else:
                    st.info("No se pudo calcular la correlación.")
            else:
                st.info("Se requieren al menos 2 variables numéricas válidas.")
        except Exception as e:
            st.error(f"Error en matriz de correlación: {e}")

with st.expander("⚖️ EDA – Balance de clases", expanded=False):
    try:
        vc = y_full.value_counts()
        fig_cls, ax_cls = plt.subplots(figsize=(min(10, len(vc)*0.8 + 2), 4))
        bars = ax_cls.bar(range(len(vc)), vc.values)
        ax_cls.set_xticks(range(len(vc)))
        ax_cls.set_xticklabels([str(x) for x in vc.index], rotation=45)
        ax_cls.set_xlabel("Clase")
        ax_cls.set_ylabel("Conteo")
        ax_cls.set_title("Distribución de clases (y)")

        # Add value labels on bars
        for bar, value in zip(bars, vc.values):
            height = bar.get_height()
            ax_cls.text(bar.get_x() + bar.get_width()/2., height,
                       f'{value}', ha='center', va='bottom')

        plt.tight_layout()
        st.pyplot(fig_cls)
        safe_plot_close()
    except Exception as e:
        st.error(f"Error en balance de clases: {e}")

# ==========================
# SPLIT + ENCODING DE ETIQUETAS
# ==========================
try:
    le = LabelEncoder()
    y_enc = le.fit_transform(y_full)
    class_names = list(le.classes_)

    X_train, X_test, y_train, y_test = train_test_split(
        X_full, y_enc, test_size=test_size, stratify=y_enc, random_state=int(random_state)
    )

    st.info(f"Train: {X_train.shape} – Test: {X_test.shape} – Clases: {class_names}")

except Exception as e:
    st.error(f"Error en split de datos: {e}")
    st.stop()

# ==========================
# PIPELINES DE PREPROCESAMIENTO
# ==========================
try:
    num_pipe = Pipeline(steps=[
        ("imp", SimpleImputer(strategy="median")),
        ("sc", StandardScaler()),
        ("pca", "passthrough"),  # alternable en la búsqueda
    ])

    cat_pipe = Pipeline(steps=[
        ("imp", SimpleImputer(strategy="most_frequent")),
        ("oh", make_ohe()),
        ("svd", "passthrough"),   # alternable (TruncatedSVD ~ MCA)
    ])

    transformers = []
    if len(num_cols) > 0:
        transformers.append(("num", num_pipe, num_cols))
    if len(cat_cols) > 0:
        transformers.append(("cat", cat_pipe, cat_cols))

    if not transformers:
        st.error("No hay columnas válidas para procesar.")
        st.stop()

    pre = ColumnTransformer(transformers=transformers)

    selector = SelectFromModel(
        ExtraTreesClassifier(random_state=int(random_state), n_estimators=100), threshold="median"
    )

    # Selección de balanceador
    cat_idx = [i for i, col in enumerate(X_full.columns) if col in cat_cols]
    if len(cat_idx) == 0:
        smote_like = SMOTE(random_state=int(random_state))
    else:
        smote_like = SMOTENC(categorical_features=cat_idx, random_state=int(random_state))

except Exception as e:
    st.error(f"Error configurando preprocesamiento: {e}")
    st.stop()

# ==========================
# MODELOS Y PIPELINES
# ==========================
try:
    # Modelos base
    modelos = {}
    if "RandomForest" in modelos_seleccionados:
        modelos["RandomForest"] = RandomForestClassifier(random_state=int(random_state), n_jobs=-1)
    if "ExtraTrees" in modelos_seleccionados:
        modelos["ExtraTrees"] = ExtraTreesClassifier(random_state=int(random_state), n_jobs=-1)
    if "HistGradientBoosting" in modelos_seleccionados:
        modelos["HistGradientBoosting"] = HistGradientBoostingClassifier(random_state=int(random_state))
    if "LogisticRegression" in modelos_seleccionados:
        modelos["LogisticRegression"] = LogisticRegression(max_iter=1000, random_state=int(random_state))
    if "SVM_Linear" in modelos_seleccionados:
        modelos["SVM_Linear"] = SVC(kernel="linear", probability=True, max_iter=1000, random_state=int(random_state))

    pipes = {}
    for nombre, clf in modelos.items():
        if nombre == "HistGradientBoosting":
            def _to_dense(X):
                return X.toarray() if hasattr(X, "toarray") else X
            densify = FunctionTransformer(_to_dense, accept_sparse=True)
            pipe = ImbPipeline(steps=[
                ("bal", "passthrough"),
                ("pre", pre),
                ("densify", densify),
                ("selector", selector if usar_selector else "passthrough"),
                ("clf", clf),
            ])
        else:
            pipe = ImbPipeline(steps=[
                ("bal", "passthrough"),
                ("pre", pre),
                ("selector", selector if usar_selector else "passthrough"),
                ("clf", clf),
            ])
        pipes[nombre] = pipe

except Exception as e:
    st.error(f"Error configurando modelos: {e}")
    st.stop()

# ==========================
# ESPACIOS DE BÚSQUEDA
# ==========================
try:
    # Common parameters
    comunes = {
        "selector": [selector if usar_selector else "passthrough"],
        "bal": ["passthrough", smote_like],
    }

    # Add preprocessing parameters only if we have the right column types
    if len(num_cols) > 0:
        comunes["pre__num__pca"] = ["passthrough", PCA(n_components=0.9, random_state=int(random_state))]
    if len(cat_cols) > 0:
        comunes["pre__cat__svd"] = ["passthrough", TruncatedSVD(n_components=min(50, len(cat_cols)), random_state=int(random_state))]

    if usar_selector:
        comunes["selector__threshold"] = ["median", "1.5*median"]

    param_grids = {}

    # RandomForest parameters
    if "RandomForest" in modelos:
        param_grids["RandomForest"] = {
            **comunes,
            "clf__n_estimators": [50, 100],
            "clf__max_depth": [None, 10],
            "clf__min_samples_split": [2, 5],
            "clf__class_weight": [None, "balanced"],
        }

    # ExtraTrees parameters
    if "ExtraTrees" in modelos:
        param_grids["ExtraTrees"] = {
            **comunes,
            "clf__n_estimators": [50, 100],
            "clf__max_depth": [None, 10],
            "clf__min_samples_split": [2, 5],
            "clf__class_weight": [None, "balanced"],
        }

    # HistGradientBoosting parameters
    if "HistGradientBoosting" in modelos:
        hist_params = {k: v for k, v in comunes.items()}
        param_grids["HistGradientBoosting"] = {
            **hist_params,
            "clf__max_iter": [50, 100],
            "clf__learning_rate": [0.1, 0.2],
            "clf__max_depth": [None, 5],
        }

    # LogisticRegression parameters
    if "LogisticRegression" in modelos:
        param_grids["LogisticRegression"] = {
            **comunes,
            "clf__C": [0.1, 1.0, 10.0],
            "clf__penalty": ["l1", "l2"],
            "clf__solver": ["liblinear"],
            "clf__class_weight": [None, "balanced"],
        }

    # SVM parameters
    if "SVM_Linear" in modelos:
        param_grids["SVM_Linear"] = {
            **comunes,
            "clf__C": [0.1, 1.0, 10.0],
            "clf__class_weight": [None, "balanced"],
        }

    # Ajustes: desactivar balanceo para árboles (memoria) y PCA num en árboles
    for nombre in ["RandomForest", "ExtraTrees", "HistGradientBoosting"]:
        if nombre in param_grids:
            param_grids[nombre]["bal"] = ["passthrough"]
            if "pre__num__pca" in param_grids[nombre]:
                param_grids[nombre]["pre__num__pca"] = ["passthrough"]

    # Disable SMOTE for linear models if requested
    if not permitir_smote_lineales:
        for nombre in ["LogisticRegression", "SVM_Linear"]:
            if nombre in param_grids:
                param_grids[nombre]["bal"] = ["passthrough"]

except Exception as e:
    st.error(f"Error configurando espacios de búsqueda: {e}")
    st.stop()

# ==========================
# ENTRENAMIENTO Y EVALUACIÓN
# ==========================
if len(modelos_seleccionados) == 0:
    st.warning("Selecciona al menos un modelo para entrenar.")
    st.stop()

cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=int(random_state))
resultados = []
mejores = {}

if st.button("🚀 Entrenar y evaluar"):
    st.write("### Entrenamiento")

    if NO_STREAMLIT:
        print("Iniciando entrenamiento automático en modo CLI...")

    barra = st.progress(0.0, text="Iniciando...")
    total = len(modelos_seleccionados)

    for i, nombre in enumerate(modelos_seleccionados, start=1):
        try:
            st.write(f"#### Entrenando {nombre} ({i}/{total})")
            t0 = time.time()

            pipe = pipes[nombre]
            grid = param_grids.get(nombre, {"clf__random_state": [int(random_state)]})

            # Ensure we have valid parameter grid
            if not grid:
                grid = {"clf__random_state": [int(random_state)]}

            search = RandomizedSearchCV(
                estimator=pipe,
                param_distributions=grid,
                n_iter=min(n_iter, 20),  # Cap iterations to prevent excessive runtime
                scoring="f1_macro",
                cv=cv,
                n_jobs=1,  # Use single job to avoid memory issues
                random_state=int(random_state),
                verbose=0,  # Reduce verbosity
                error_score='raise',  # Changed from np.nan to 'raise' for better error handling
            )

            with st.spinner(f"Ajustando {nombre}..."):
                search.fit(X_train, y_train)

            best_est = search.best_estimator_
            mejores[nombre] = best_est
            f1_cv = search.best_score_
            elapsed = time.time() - t0

            # Predictions
            y_pred = best_est.predict(X_test)
            f1_t = f1_score(y_test, y_pred, average="macro")

            resultados.append({
                "Modelo": nombre,
                "CV_F1_macro": f1_cv,
                "Test_F1_macro": f1_t,
                "Tiempo_s": elapsed,
                "Best_Params": str(search.best_params_),
            })

            st.subheader(f"Resultados – {nombre}")

            # Classification report
            try:
                report = classification_report(y_test, y_pred, target_names=class_names, zero_division=0)
                st.code(report)
            except Exception as e:
                st.warning(f"Error generando reporte de clasificación: {e}")

            # Confusion Matrix
            try:
                cm = confusion_matrix(y_test, y_pred)
                fig_cm, ax_cm = plt.subplots(figsize=(6, 5))
                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
                disp.plot(cmap="Blues", values_format='d', ax=ax_cm)
                ax_cm.set_title(f"Matriz de Confusión – {nombre}")
                plt.tight_layout()
                st.pyplot(fig_cm)
                safe_plot_close()
            except Exception as e:
                st.warning(f"Error generando matriz de confusión: {e}")

            # ROC/PR curves
            try:
                n_classes = len(class_names)
                if n_classes > 2:
                    y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))
                else:
                    y_test_bin = y_test.reshape(-1, 1)

                y_score = None
                if hasattr(best_est, "predict_proba"):
                    try:
                        y_score = best_est.predict_proba(X_test)
                    except Exception:
                        pass

                if y_score is None and hasattr(best_est, "decision_function"):
                    try:
                        y_score = best_est.decision_function(X_test)
                        # For binary classification, decision_function returns 1D array
                        if y_score.ndim == 1 and n_classes == 2:
                            y_score = np.column_stack([-y_score, y_score])
                    except Exception:
                        pass

                if y_score is not None and y_score.shape[1] >= n_classes:
                    # ROC curves
                    fig_roc, ax_roc = plt.subplots(figsize=(7, 6))
                    plotted = 0

                    for j in range(n_classes):
                        try:
                            if n_classes > 2:
                                y_true_j = y_test_bin[:, j]
                                y_score_j = y_score[:, j]
                            else:
                                y_true_j = (y_test == j).astype(int)
                                y_score_j = y_score[:, j]

                            if len(np.unique(y_true_j)) >= 2:
                                fpr, tpr, _ = roc_curve(y_true_j, y_score_j)
                                roc_auc = auc(fpr, tpr)
                                ax_roc.plot(fpr, tpr, lw=2, label=f"{class_names[j]} (AUC={roc_auc:.3f})")
                                plotted += 1
                        except Exception:
                            continue

                    if plotted > 0:
                        ax_roc.plot([0, 1], [0, 1], linestyle="--", lw=1, color='gray')
                        ax_roc.set_xlim([0.0, 1.0])
                        ax_roc.set_ylim([0.0, 1.05])
                        ax_roc.set_xlabel("Tasa de falsos positivos")
                        ax_roc.set_ylabel("Tasa de verdaderos positivos")
                        ax_roc.set_title(f"Curvas ROC (OvR) – {nombre}")
                        ax_roc.legend(loc="lower right")
                        plt.tight_layout()
                        st.pyplot(fig_roc)
                        safe_plot_close()

                    # PR curves
                    if graficar_pr:
                        fig_pr, ax_pr = plt.subplots(figsize=(7, 6))
                        plotted_pr = 0

                        for j in range(n_classes):
                            try:
                                if n_classes > 2:
                                    y_true_j = y_test_bin[:, j]
                                    y_score_j = y_score[:, j]
                                else:
                                    y_true_j = (y_test == j).astype(int)
                                    y_score_j = y_score[:, j]

                                if len(np.unique(y_true_j)) >= 2:
                                    precision, recall, _ = precision_recall_curve(y_true_j, y_score_j)
                                    ap = average_precision_score(y_true_j, y_score_j)
                                    ax_pr.plot(recall, precision, lw=2, label=f"{class_names[j]} (AP={ap:.3f})")
                                    plotted_pr += 1
                            except Exception:
                                continue

                        if plotted_pr > 0:
                            ax_pr.set_xlim([0.0, 1.0])
                            ax_pr.set_ylim([0.0, 1.05])
                            ax_pr.set_xlabel("Recall")
                            ax_pr.set_ylabel("Precision")
                            ax_pr.set_title(f"Curvas Precision–Recall (OvR) – {nombre}")
                            ax_pr.legend(loc="lower left")
                            plt.tight_layout()
                            st.pyplot(fig_pr)
                            safe_plot_close()
                else:
                    st.info(f"No fue posible trazar ROC/PR para {nombre}: el modelo no expone probabilidades adecuadas.")

            except Exception as e:
                st.warning(f"Error generando curvas ROC/PR para {nombre}: {e}")

            # Update progress
            progress = i / total
            barra.progress(progress, text=f"Completado {i}/{total}")

        except Exception as e:
            st.error(f"Error entrenando {nombre}: {e}")
            continue

    # Results summary
    if resultados:
        df_res = pd.DataFrame(resultados).sort_values("Test_F1_macro", ascending=False)
        st.success("Entrenamiento finalizado")
        st.write("### Resumen de modelos (ordenado por F1_macro en Test)")
        st.dataframe(df_res, use_container_width=True)

        if len(df_res) > 0:
            top = df_res.iloc[0]
            st.info(
                f"**Mejor modelo**: {top['Modelo']} – F1_macro Test = {top['Test_F1_macro']:.4f} (CV={top['CV_F1_macro']:.4f})\n\n"
                f"**Tiempo**: {top['Tiempo_s']:.1f}s\n\n"
                f"**Hiperparámetros**: {top['Best_Params'][:200]}{'...' if len(top['Best_Params']) > 200 else ''}"
            )

        # ==========================
        # PRUEBAS ADICIONALES (sanidad post-entrenamiento)
        # ==========================
        try:
            assert not df_res["Test_F1_macro"].isna().any(), "F1_macro en test no debe contener NaN."
            assert df_res["Test_F1_macro"].between(0, 1).all(), "F1_macro fuera de rango [0,1]."
            assert len(df_res) <= len(modelos_seleccionados), "Demasiadas filas en el resumen."

            # Confusion matrix dimensions check for best model
            if mejores:
                best_model_name = top['Modelo']
                if best_model_name in mejores:
                    cm = confusion_matrix(y_test, mejores[best_model_name].predict(X_test))
                    assert cm.shape == (len(class_names), len(class_names)), "Dimensiones inesperadas en la matriz de confusión."

            st.success("✅ Todas las pruebas de sanidad pasaron correctamente.")

        except AssertionError as e:
            st.warning(f"⚠️ Self-test resultados: {e}")
        except Exception as e:
            st.warning(f"⚠️ Error en self-test: {e}")
    else:
        st.error("No se pudieron entrenar modelos. Revisa los parámetros y los datos.")

else:
    st.info("👈 Configura las opciones en la barra lateral y presiona **🚀 Entrenar y evaluar** para comenzar.")

# ==========================
# PRUEBAS BÁSICAS (sanidad previas)
# ==========================
try:
    assert len(class_names) >= 2, "Se esperaban ≥2 clases. Verifica la columna objetivo del UCI Repo."
    assert X_train.shape[0] > 0 and X_test.shape[0] > 0, "Split vacío: revisa el tamaño de test."

    # Test preprocessing pipeline
    if len(X_train) > 0:
        sample_data = X_train.iloc[:min(10, len(X_train))]
        _ = pre.fit(X_train).transform(sample_data)

    # Verify balancer type
    if len(cat_cols) == 0:
        assert isinstance(smote_like, SMOTE), "Se esperaba SMOTE cuando no hay variables categóricas."
    else:
        assert isinstance(smote_like, SMOTENC), "Se esperaba SMOTENC cuando hay variables categóricas."

    # Basic data validation
    assert not X_train.empty, "Conjunto de entrenamiento vacío."
    assert not X_test.empty, "Conjunto de test vacío."
    assert len(y_train) == len(X_train), "Desajuste en tamaños de X_train y y_train."
    assert len(y_test) == len(X_test), "Desajuste en tamaños de X_test y y_test."

except AssertionError as e:
    st.warning(f"⚠️ Self-test inicial: {e}")
except Exception as e:
    st.warning(f"⚠️ Self-test preprocesamiento: {e}")

st.write("---")
st.caption("🔧 **Estado del sistema**: App lista. Selecciona modelos y parámetros para comenzar el entrenamiento.")