# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aOt9CRonp7QMS4ekEucePpOVlGGBWOWL
"""

# app.py ‚Äî Safe-boot Streamlit wrapper for the Diabetes ML pipeline
# Defers heavy work until "Run pipeline" button is clicked.
# Shows clear, on-screen errors instead of crashing the server on startup.

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import streamlit as st

import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.feature_selection import chi2, f_classif
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import (
    classification_report, confusion_matrix, ConfusionMatrixDisplay,
    roc_curve, auc
)
from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import ADASYN, SMOTE

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.utils.class_weight import compute_class_weight
from scipy.stats import randint, uniform

# -------------------------
# Page + error behavior
# -------------------------
st.set_page_config(page_title="Diabetes ML ‚Äì Interactive", page_icon="ü©∫", layout="wide")
st.title("ü©∫ Diabetes ML ‚Äì Interactive Pipeline")
st.caption("Safe-boot: press **Run pipeline** to execute. Errors will appear here, not crash the app.")
st.set_option("client.showErrorDetails", True)

def show_and_stop(msg: str, exc: Exception = None):
    st.error(msg)
    if exc is not None:
        st.exception(exc)
    st.stop()

# Environment sanity line (renders immediately; helps debugging)
try:
    import sklearn
    st.caption(f"üîé Env ready ‚Äî numpy {np.__version__} | pandas {pd.__version__} | scikit-learn {sklearn.__version__}")
except Exception as e:
    show_and_stop("Environment imports failed. Check requirements.txt.", e)

RANDOM_STATE_DEFAULT = 42

# -------------------------
# Sidebar controls (lightweight)
# -------------------------
st.sidebar.header("‚öôÔ∏è Controls")
seed = st.sidebar.number_input("Random Seed", value=RANDOM_STATE_DEFAULT, step=1)
sample_n = st.sidebar.slider("Sample rows (for speed)", min_value=3000, max_value=100000, value=20000, step=1000)
thresh = st.sidebar.slider("Cumulative importance / variance threshold", 0.70, 0.99, 0.90, 0.01)
feat_strategy = st.sidebar.selectbox("Feature selection strategy",
                                     ["Filter (ANOVA numeric + Chi¬≤ categorical)", "Embedded (Random Forest)"])
pca_enable = st.sidebar.checkbox("Apply PCA to numeric block", value=True)
mca_enable = st.sidebar.checkbox("Apply MCA to categorical block", value=True)
safe_mode = st.sidebar.toggle("üõü Safe Mode (skip MCA & heavy search)", value=True)  # default ON for reliability
if safe_mode:
    mca_enable = False
st.sidebar.divider()
balancing = st.sidebar.selectbox("Rebalancing",
                                 ["None", "class_weight=balanced (if supported)", "SMOTE(k=3)", "ADASYN"])
model_name = st.sidebar.selectbox("Model",
                                  ["RandomForest", "ExtraTrees", "HistGradientBoosting", "LogisticRegression", "SVM_Linear"])
param_mode = st.sidebar.radio("Hyperparameter search", ["Fast (quick grids)", "Full (wider grids)"], index=0)
n_iter = st.sidebar.slider("RandomizedSearch n_iter", 5, 50, 10, 5)
cv_folds = st.sidebar.slider("CV folds", 3, 10, 3, 1)
test_size = st.sidebar.slider("Test size", 0.1, 0.4, 0.25, 0.05)
SEARCH_N_JOBS = 1 if safe_mode else -1

# Run button
col_run, col_reset = st.columns([1,1])
run_clicked = col_run.button("‚ñ∂Ô∏è Run pipeline", type="primary")
if col_reset.button("‚ôªÔ∏è Clear cache"):
    st.cache_data.clear()
    st.rerun()

# -------------------------
# Lightweight previews (no heavy work yet)
# -------------------------
with st.expander("What this will do (no compute yet)"):
    st.write(
        """
        1) Download & sample the UCI Diabetes 130-US Hospitals dataset
        2) Feature selection ‚Üí PCA/MCA (as enabled)
        3) Stratified train/test split
        4) Optional rebalancing
        5) Hyperparameter search (RandomizedSearchCV)
        6) Metrics + plots + CSV download
        """
    )

# -------------------------
# Helpers (definitions only; no heavy compute)
# -------------------------
@st.cache_data(show_spinner=False)
def load_data(seed: int, sample_n: int):
    from ucimlrepo import fetch_ucirepo
    try:
        ds = fetch_ucirepo(id=296)
    except Exception as e:
        raise RuntimeError("Could not download the UCI dataset (network or package issue).") from e

    X_raw: pd.DataFrame = ds.data.features.copy()
    y_raw: pd.DataFrame = ds.data.targets.copy()

    # Drop id-like / overly specific columns for a general demo
    id_like = [c for c in X_raw.columns if c.lower() in
               ("encounter_id", "patient_nbr", "encounterid", "patientnbr",
                "diag_1", "diag_2", "diag_3", "payer_code", "medical_specialty")]
    X_raw = X_raw.drop(columns=id_like, errors="ignore")

    # Sample for speed
    if sample_n < len(X_raw):
        rng = np.random.default_rng(seed)
        idx = rng.choice(X_raw.index, size=sample_n, replace=False)
        X_raw = X_raw.loc[idx].reset_index(drop=True)
        y_raw = y_raw.loc[idx].reset_index(drop=True)
    else:
        X_raw = X_raw.reset_index(drop=True)
        y_raw = y_raw.reset_index(drop=True)

    X_num = X_raw.select_dtypes(include=[np.number]).copy()
    X_cat = X_raw.select_dtypes(include=['object', 'category']).copy()
    return X_raw, y_raw, X_num, X_cat

def prepare_blocks(X_num: pd.DataFrame, X_cat: pd.DataFrame):
    Xnum_imp = X_num.copy()
    for c in Xnum_imp.columns:
        if not np.issubdtype(Xnum_imp[c].dtype, np.number):
            Xnum_imp[c] = pd.to_numeric(Xnum_imp[c], errors='coerce')
    Xnum_imp = Xnum_imp.fillna(Xnum_imp.median(numeric_only=True))
    Xcat_imp = X_cat.fillna('Missing').astype(str)
    return Xnum_imp, Xcat_imp

def filter_selection(Xnum_imp, Xcat_imp, y_enc, thresh):
    Xcat_dum_all = pd.get_dummies(Xcat_imp, drop_first=False, prefix_sep='=')
    chi2_scores, _ = chi2(Xcat_dum_all.fillna(0), y_enc)
    var_of_dummy = Xcat_dum_all.columns.to_series().str.split('=', n=1, expand=True)[0]
    var_importance_cat = (
        pd.DataFrame({'var': var_of_dummy, 'score': chi2_scores})
        .groupby('var', sort=False)['score']
        .sum()
        .sort_values(ascending=False)
    )
    cum_cat = var_importance_cat.cumsum() / var_importance_cat.sum()
    cat_vars_sel = cum_cat[cum_cat <= thresh].index.tolist()
    if len(cat_vars_sel) < len(var_importance_cat):
        cat_vars_sel = cat_vars_sel + [var_importance_cat.index[len(cat_vars_sel)]]
    X_cat_sel = Xcat_imp[cat_vars_sel]
    X_cat_sel_dum = pd.get_dummies(X_cat_sel, drop_first=True, prefix_sep='=')

    F_scores, _ = f_classif(Xnum_imp, y_enc)
    num_importance = pd.Series(F_scores, index=Xnum_imp.columns).sort_values(ascending=False)
    cum_num = num_importance.cumsum() / num_importance.sum()
    num_vars_sel = cum_num[cum_num <= thresh].index.tolist()
    if len(num_vars_sel) < len(num_importance):
        num_vars_sel = list(num_vars_sel) + [num_importance.index[len(num_vars_sel)]]
    X_num_sel = Xnum_imp[num_vars_sel]

    df_filtrado = pd.concat([X_num_sel.reset_index(drop=True),
                             X_cat_sel_dum.reset_index(drop=True)], axis=1)
    details = {
        "cat_vars_sel": cat_vars_sel,
        "num_vars_sel": list(num_vars_sel),
        "dummies_cat": X_cat_sel_dum.shape[1],
        "var_importance_cat": var_importance_cat,
        "num_importance": num_importance
    }
    return df_filtrado, details, X_cat_sel

def embedded_rf_selection(Xnum_imp, Xcat_imp, y_enc, thresh, seed):
    Xcat_dum_all = pd.get_dummies(Xcat_imp, drop_first=False, prefix_sep='=').fillna(0)
    X_complete = pd.concat([Xnum_imp, Xcat_dum_all], axis=1)
    rf = RandomForestClassifier(n_estimators=200, random_state=seed, n_jobs=-1)
    rf.fit(X_complete, y_enc)
    importances = rf.feature_importances_
    idx = np.argsort(importances)[::-1]
    cumsum = np.cumsum(importances[idx])
    cutoff = np.where(cumsum >= thresh)[0][0] + 1
    selected_cols = X_complete.columns[idx][:cutoff]
    df_filtrado = X_complete[selected_cols].copy()

    cat_cols = [c for c in selected_cols if c in Xcat_dum_all.columns]
    base_cats = sorted(set([c.split('=')[0] for c in cat_cols]))
    X_cat_sel = Xcat_imp[base_cats] if base_cats else pd.DataFrame(index=Xcat_imp.index)

    details = {
        "selected_features": list(selected_cols),
        "cutoff_features": cutoff,
        "rf_importances_sorted": pd.Series(importances[idx], index=X_complete.columns[idx])
    }
    return df_filtrado, details, X_cat_sel

def pca_mca_blocks(df_filtrado, X_cat_sel, apply_pca, apply_mca, thresh, seed):
    parts, info = [], {}

    if apply_pca:
        df_num = df_filtrado.select_dtypes(include=[np.number])
        if df_num.shape[1] > 0:
            scaler = StandardScaler()
            Xn = scaler.fit_transform(df_num)
            p = PCA(random_state=seed).fit(Xn)
            cum = np.cumsum(p.explained_variance_ratio_)
            r = int(np.searchsorted(cum, thresh) + 1)
            Xp = p.transform(Xn)[:, :r]
            parts.append(pd.DataFrame(Xp, columns=[f"PC{i+1}" for i in range(r)], index=df_filtrado.index))
            info["pca_kept"], info["pca_var"] = r, float(cum[r-1])
        else:
            info["pca_kept"], info["pca_var"] = 0, 0.0
    else:
        info["pca_kept"], info["pca_var"] = 0, 0.0

    if apply_mca and X_cat_sel is not None and X_cat_sel.shape[1] > 0:
        try:
            import mca as mca_lib
            Xcat_dum = pd.get_dummies(X_cat_sel.fillna('Missing').astype(str), drop_first=False, prefix_sep='=')
            m = mca_lib.MCA(Xcat_dum, benzecri=True)
            sv = m.s; eig = sv ** 2; expl = eig / eig.sum(); cum = np.cumsum(expl)
            r = int(np.searchsorted(cum, thresh) + 1)
            try:
                F = m.fs_r(N=r)
            except Exception:
                F = m.fs_r()[:, :r]
            parts.append(pd.DataFrame(F, columns=[f"MCA{i+1}" for i in range(r)], index=X_cat_sel.index))
            info["mca_kept"], info["mca_var"] = r, float(cum[r-1])
        except Exception as e:
            st.warning(f"MCA skipped (error: {e}).")
            info["mca_kept"], info["mca_var"] = 0, 0.0
    else:
        if apply_mca:
            st.caption("MCA disabled: no categorical columns selected.")
        info.setdefault("mca_kept", 0); info.setdefault("mca_var", 0.0)

    df_combined = pd.concat(parts, axis=1) if parts else df_filtrado.copy()
    return df_combined, info

def get_models(seed):
    return {
        "RandomForest": RandomForestClassifier(random_state=seed),
        "ExtraTrees": ExtraTreesClassifier(random_state=seed),
        "HistGradientBoosting": HistGradientBoostingClassifier(random_state=seed),
        "LogisticRegression": LogisticRegression(random_state=seed, max_iter=1000, solver='liblinear'),
        "SVM_Linear": SVC(kernel='linear', random_state=seed, probability=True, max_iter=1000),
    }

def get_param_spaces(mode):
    full = {
        "RandomForest": {
            'classifier__n_estimators': randint(100, 400),
            'classifier__max_depth': [10, 20, 30, None],
            'classifier__min_samples_split': randint(2, 10),
            'classifier__min_samples_leaf': randint(1, 5),
            'classifier__max_features': ['sqrt', 'log2']
        },
        "ExtraTrees": {
            'classifier__n_estimators': randint(100, 400),
            'classifier__max_depth': [10, 20, 30, None],
            'classifier__min_samples_split': randint(2, 10),
            'classifier__min_samples_leaf': randint(1, 5),
            'classifier__max_features': ['sqrt', 'log2']
        },
        "HistGradientBoosting": {
            'classifier__max_iter': randint(50, 250),
            'classifier__learning_rate': uniform(0.05, 0.25),
            'classifier__max_depth': randint(3, 10),
            'classifier__max_leaf_nodes': [31, 50, 100, 200]
        },
        "LogisticRegression": {
            'classifier__C': uniform(0.1, 10),
            'classifier__penalty': ['l1', 'l2'],
            'classifier__solver': ['liblinear', 'saga'],
            'classifier__max_iter': [1000, 2000]
        },
        "SVM_Linear": {
            'classifier__C': uniform(0.1, 10),
            'classifier__max_iter': [1000, 2000]
        }
    }
    fast = {
        "RandomForest": {'classifier__n_estimators': [100, 200], 'classifier__max_depth': [10, 20]},
        "ExtraTrees":  {'classifier__n_estimators': [100, 200], 'classifier__max_depth': [10, 20]},
        "HistGradientBoosting": {'classifier__max_iter': [60, 120], 'classifier__learning_rate': [0.1, 0.2]},
        "LogisticRegression": {'classifier__C': [0.1, 1, 10], 'classifier__penalty': ['l2'], 'classifier__solver': ['liblinear']},
        "SVM_Linear": {'classifier__C': [0.1, 1, 10]}
    }
    return fast if mode.startswith("Fast") else full

def build_preprocessor(columns):
    numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),
                                          ('scaler', StandardScaler())])
    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, columns)])
    return preprocessor

def apply_balancing_choice(balancing, model, y_train):
    steps, info = [], ""
    if balancing == "SMOTE(k=3)":
        steps.append(('balancer', SMOTE(random_state=RANDOM_STATE_DEFAULT, k_neighbors=3))); info = "SMOTE(k=3)"
    elif balancing == "ADASYN":
        steps.append(('balancer', ADASYN(random_state=RANDOM_STATE_DEFAULT))); info = "ADASYN"
    elif balancing == "class_weight=balanced (if supported)":
        if hasattr(model, "class_weight"):
            weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
            model.set_params(class_weight=dict(zip(np.unique(y_train), weights)))
            info = "class_weight=balanced"
        else:
            info = "class_weight requested but not supported; ignored"
    else:
        info = "None"
    return steps, info

# -------------------------
# Heavy work inside button
# -------------------------
if run_clicked:
    try:
        with st.spinner("Loading dataset..."):
            X_raw, y_raw, X_num, X_cat = load_data(seed, sample_n)
    except Exception as e:
        show_and_stop("Dataset load failed.", e)

    # Encode target
    try:
        y_series = y_raw.iloc[:, 0].astype(str)
    except Exception as e:
        show_and_stop("Could not read target column from dataset.", e)

    le_original = LabelEncoder().fit(y_series)
    y_enc = le_original.transform(y_series)
    class_names = list(le_original.classes_)

    st.subheader("Dataset snapshot")
    c1, c2, c3 = st.columns(3)
    c1.metric("Rows", len(X_raw))
    c2.metric("Numeric cols", X_num.shape[1])
    c3.metric("Categorical cols", X_cat.shape[1])
    st.dataframe(pd.concat([X_num.head(3), X_cat.head(3)], axis=1))
    with st.expander("Class distribution (full sample)"):
        st.write(pd.Series(y_series).value_counts())

    # Preprocess + feature selection
    Xnum_imp, Xcat_imp = prepare_blocks(X_num, X_cat)
    with st.spinner("Selecting features..."):
        if feat_strategy.startswith("Filter"):
            df_filtrado, details, X_cat_sel_for_mca = filter_selection(Xnum_imp, Xcat_imp, y_enc, thresh)
        else:
            df_filtrado, details, X_cat_sel_for_mca = embedded_rf_selection(Xnum_imp, Xcat_imp, y_enc, thresh, seed)

    st.subheader("Selected features")
    st.write(f"Resulting matrix: {df_filtrado.shape[0]} rows √ó {df_filtrado.shape[1]} columns")
    if feat_strategy.startswith("Filter"):
        st.caption(f"üßÆ Numeric kept: {len(details['num_vars_sel'])} | Categorical kept: {len(details['cat_vars_sel'])} ‚Üí dummies: {details['dummies_cat']}")
    else:
        st.caption(f"üß† Embedded RF kept top {len(details['selected_features'])} features (‚â• {int(thresh*100)}% cumulative importance)")

    with st.expander("Top importances / stats"):
        if feat_strategy.startswith("Filter"):
            st.write("Top categorical (Chi¬≤ aggregated):"); st.write(details['var_importance_cat'].head(15))
            st.write("Top numeric (ANOVA F):"); st.write(details['num_importance'].head(15))
        else:
            st.write("Top RF importances:"); st.write(details['rf_importances_sorted'].head(25))

    with st.spinner("Computing PCA/MCA blocks..."):
        df_combined, decomp_info = pca_mca_blocks(df_filtrado, X_cat_sel_for_mca, pca_enable, mca_enable, thresh, seed)

    st.subheader("Dimensionality reduction")
    st.write(f"Combined matrix used for training: {df_combined.shape}")
    colA, colB = st.columns(2)
    colA.caption(f"PCA kept: {decomp_info.get('pca_kept', 0)} comps (var‚âà{decomp_info.get('pca_var', 0):.3f})")
    colB.caption(f"MCA kept: {decomp_info.get('mca_kept', 0)} dims (var‚âà{decomp_info.get('mca_var', 0):.3f})")

    if df_combined.shape[1] >= 2:
        with st.expander("PC1 vs PC2 / MCA1 vs MCA2 scatter (first two columns of combined)"):
            fig, ax = plt.subplots(figsize=(5.5, 5))
            ax.scatter(df_combined.iloc[:, 0], df_combined.iloc[:, 1], c=y_enc, alpha=0.6)
            ax.set_xlabel(df_combined.columns[0]); ax.set_ylabel(df_combined.columns[1])
            ax.set_title("First two components (combined)")
            st.pyplot(fig)

    # Train / test split
    X_train, X_test, y_train, y_test = train_test_split(
        df_combined, y_enc, test_size=test_size, stratify=y_enc, random_state=seed
    )

    st.subheader("Train/Test split")
    c1, c2 = st.columns(2)
    c1.write(f"X_train: {X_train.shape}, y_train: {pd.Series(y_train).value_counts().to_dict()}")
    c2.write(f"X_test: {X_test.shape}, y_test: {pd.Series(y_test).value_counts().to_dict()}")

    # Build pipeline + search
    models = get_models(seed)
    clf = models[model_name]
    preprocessor = build_preprocessor(df_combined.columns.tolist())
    balance_steps, balance_info = apply_balancing_choice(balancing, clf, y_train)
    Pipe = ImbPipeline if any(s[0] == 'balancer' for s in balance_steps) else Pipeline
    pipeline = Pipe([('preprocessor', preprocessor), *balance_steps, ('classifier', clf)])

    param_spaces = get_param_spaces(param_mode)
    param_grid = dict(param_spaces.get(model_name, {}))  # copy

    # prune unsupported params (prevents search crash)
    supported = set(clf.get_params().keys())
    pruned_grid = {}
    for k, v in param_grid.items():
        suffix = k.split("__", 1)[-1]
        if suffix in supported:
            pruned_grid[k] = v
        else:
            st.warning(f"Dropping unsupported param for {model_name}: {k}")
    param_grid = pruned_grid

    # soften grids in safe mode
    if safe_mode:
        for k in list(param_grid.keys()):
            vals = param_grid[k]
            if isinstance(vals, list):
                param_grid[k] = vals[:2]
        n_iter_local = min(n_iter, 5)
        cv_folds_local = min(cv_folds, 3)
        n_jobs_local = 1
    else:
        n_iter_local, cv_folds_local, n_jobs_local = n_iter, cv_folds, SEARCH_N_JOBS

    # Train
    st.subheader("Training")
    with st.spinner(f"Training {model_name} ({param_mode})..."):
        if param_grid:
            try:
                search = RandomizedSearchCV(
                    pipeline,
                    param_distributions=param_grid,
                    n_iter=n_iter_local,
                    cv=cv_folds_local,
                    scoring='f1_macro',
                    random_state=seed,
                    n_jobs=n_jobs_local,
                    error_score='raise'
                )
                search.fit(X_train, y_train)
                best_model = search.best_estimator_
                best_cv = search.best_score_
                st.success(f"Best CV f1_macro: {best_cv:.4f}")
                st.caption(f"Best params: {search.best_params_}")
            except Exception as e:
                st.warning("Hyperparameter search failed; fitting default pipeline.")
                st.exception(e)
                pipeline.fit(X_train, y_train)
                best_model = pipeline
        else:
            pipeline.fit(X_train, y_train)
            best_model = pipeline

    # Evaluate
    st.subheader("Evaluation on Test Set")
    try:
        y_pred = best_model.predict(X_test)
    except Exception as e:
        show_and_stop("Prediction failed.", e)

    proba_ok, y_proba = hasattr(best_model, "predict_proba"), None
    if proba_ok:
        try:
            y_proba = best_model.predict_proba(X_test)
        except Exception as e:
            st.warning(f"predict_proba failed: {e}")

    report = classification_report(y_test, y_pred, target_names=class_names, output_dict=False, zero_division=0)
    st.text(report)

    cm = confusion_matrix(y_test, y_pred)
    fig_cm, ax_cm = plt.subplots(figsize=(6, 5))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    disp.plot(cmap="Blues", values_format='d', ax=ax_cm, colorbar=False)
    ax_cm.set_title(f"Confusion Matrix ‚Äì {model_name}")
    st.pyplot(fig_cm)

    if y_proba is not None and len(np.unique(y_test)) == 2:
        fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])
        roc_auc = auc(fpr, tpr)
        fig_roc, ax_roc = plt.subplots(figsize=(6, 5))
        ax_roc.plot(fpr, tpr, lw=2, label=f"AUC={roc_auc:.3f}")
        ax_roc.plot([0, 1], [0, 1], linestyle="--")
        ax_roc.set_xlabel("FPR"); ax_roc.set_ylabel("TPR"); ax_roc.legend()
        ax_roc.set_title("ROC Curve (binary only)")
        st.pyplot(fig_roc)
    else:
        st.caption("ROC not shown (multi-class or no predict_proba).")

    # Download
    out = pd.DataFrame({"y_true": y_test, "y_pred": y_pred})
    if y_proba is not None:
        out = pd.concat([out.reset_index(drop=True),
                         pd.DataFrame(y_proba, columns=[f"p_{c}" for c in range(y_proba.shape[1])])], axis=1)
    st.download_button("‚¨áÔ∏è Download test predictions (CSV)",
                       data=out.to_csv(index=False).encode("utf-8"),
                       file_name="predictions_test.csv", mime="text/csv")

    st.markdown("---")
    st.caption(f"Model: **{model_name}** | Balancing: **{balancing}** | Search: **{param_mode}** | Seed: **{seed}** | Safe Mode: **{safe_mode}**")
else:
    st.info("Press **Run pipeline** to execute the workflow. (This keeps the server light and avoids startup crashes.)")