# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1az9rbFwqphO6aDAhL0OhFleAJdR5ciJS
"""

# -*- coding: utf-8 -*-
# ========================================
# App Streamlit ‚Äî Diabetes UCI (id=296)
# Paso 1: Carga de datos + EDA b√°sico (sin fugas)
# ========================================

import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import streamlit as st

from ucimlrepo import fetch_ucirepo
from sklearn.preprocessing import LabelEncoder

# -------------------------------
# Configuraci√≥n de la p√°gina
# -------------------------------
st.set_page_config(
    page_title="Diabetes UCI ‚Äî ML sin fugas",
    page_icon="ü©∫",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ü©∫ Diabetes UCI ‚Äî ML sin fugas (Paso 1)")
st.caption("Carga de datos desde UCI, limpieza b√°sica segura y EDA inicial. "
           "En los siguientes pasos a√±adiremos el pipeline, CV y modelos.")

RANDOM_STATE = 42

# ---------------------------------------------
# Sidebar: Par√°metros de muestreo (opcional)
# ---------------------------------------------
st.sidebar.header("‚öôÔ∏è Configuraci√≥n")
sample_on = st.sidebar.checkbox("Activar muestreo para pruebas r√°pidas", value=True)
N_SAMPLE = st.sidebar.number_input(
    "Tama√±o de muestra (si se activa muestreo)",
    min_value=1000, max_value=200000, value=50000, step=1000
)
st.sidebar.info(
    "Usa el muestreo si el dataset completo te va lento en tu entorno. "
    "Siempre cargamos desde el UCI repo para evitar usar archivos locales."
)

# ------------------------------------------------
# Funciones auxiliares: carga y limpieza ‚Äúsegura‚Äù
# ------------------------------------------------
@st.cache_data(show_spinner=True)
def load_data(random_state: int, n_sample: int | None):
    # 1) Cargar datos crudos desde UCI
    ds = fetch_ucirepo(id=296)
    X = ds.data.features.copy()
    y = ds.data.targets.copy().iloc[:, 0].astype(str)

    # 2) Eliminar columnas con alto riesgo de fuga / cardinalidad
    id_like = [c for c in X.columns if c.lower() in (
        "encounter_id", "patient_nbr", "encounterid", "patientnbr",
        "diag_1", "diag_2", "diag_3", "payer_code", "medical_specialty"
    )]
    X = X.drop(columns=id_like, errors="ignore")

    # 3) Muestreo opcional (aleatorio estricto por filas, sin aprender nada)
    if n_sample is not None and len(X) > n_sample:
        rng = np.random.RandomState(random_state)
        idx = rng.choice(X.index, size=n_sample, replace=False)
        X = X.loc[idx].reset_index(drop=True)
        y = y.loc[idx].reset_index(drop=True)
    else:
        X = X.reset_index(drop=True)
        y = y.reset_index(drop=True)

    # 4) Tipos de columna
    num_cols = X.select_dtypes(include=np.number).columns.tolist()
    cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()

    # 5) Codificaci√≥n de y (solo para evaluaci√≥n posterior; aqu√≠ no entrenamos nada)
    le = LabelEncoder().fit(y)
    y_enc = le.transform(y)
    class_names = le.classes_

    return X, y, y_enc, num_cols, cat_cols, class_names

with st.spinner("Descargando dataset desde UCI y preparando vista inicial..."):
    X, y, y_enc, num_cols, cat_cols, class_names = load_data(
        RANDOM_STATE,
        N_SAMPLE if sample_on else None
    )
    time.sleep(0.2)

# -------------------------------
# EDA inicial
# -------------------------------
st.subheader("üì¶ Resumen del dataset (vista r√°pida)")
c1, c2, c3, c4 = st.columns(4)
c1.metric("Filas", f"{len(X):,}")
c2.metric("Columnas", f"{X.shape[1]:,}")
c3.metric("Num√©ricas", f"{len(num_cols):,}")
c4.metric("Categ√≥ricas", f"{len(cat_cols):,}")

with st.expander("üëÄ Ver primeras filas (X)"):
    st.dataframe(X.head(20), use_container_width=True)

with st.expander("üî§ Variable objetivo (y) ‚Äî primeras filas"):
    st.write(pd.DataFrame({"y": y}).head(20))

st.markdown("**Clases:** " + ", ".join([f"`{c}`" for c in class_names]))

# Distribuci√≥n de clases
st.subheader("üìä Distribuci√≥n de clases")
fig, ax = plt.subplots(figsize=(6, 4))
(pd.Series(y)
   .value_counts()
   .sort_index()
   .plot(kind="bar", ax=ax))
ax.set_xlabel("Clase")
ax.set_ylabel("Conteo")
ax.set_title("Frecuencia por clase")
st.pyplot(fig, clear_figure=True)

# Tipolog√≠a de variables
st.subheader("üß© Tipos de variables")
col1, col2 = st.columns(2)
with col1:
    st.write("**Num√©ricas**")
    if len(num_cols):
        st.code("\n".join(num_cols), language="text")
    else:
        st.info("No se detectaron columnas num√©ricas.")

with col2:
    st.write("**Categ√≥ricas**")
    if len(cat_cols):
        st.code("\n".join(cat_cols), language="text")
    else:
        st.info("No se detectaron columnas categ√≥ricas.")

# NaNs por columna
st.subheader("üßº Valores faltantes por columna")
na_counts = X.isna().sum().sort_values(ascending=False)
st.dataframe(na_counts.to_frame("n_missing"), use_container_width=True)

st.success(
    "Listo el Paso 1 ‚úÖ ‚Äî Ya cargamos datos **desde UCI**, aplicamos limpieza segura, "
    "mostramos la distribuci√≥n de clases y la tipolog√≠a de variables.\n\n"
    "‚û°Ô∏è Siguiente: armaremos el **pipeline de preprocesamiento** (imputers, escalado, "
    "one-hot/encodings) y el **esquema de CV sin fugas** antes de entrenar modelos."
)